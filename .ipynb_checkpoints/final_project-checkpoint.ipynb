{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "By2Q8iXcNQKd"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ZTv1HuPHSWR"
   },
   "source": [
    "Handwritten mathematical-expression recognition (HMR) is the problem that attempts to take a handwritten mathematical formula such as:\n",
    "\n",
    "[Insert image]\n",
    "\n",
    "and convert it into some computer-readable format.  The nearly-ubiquitous format is LaTeX encoding, which is how modern math papers are typically written.  In LaTeX, the above image would be expressed as \n",
    "\n",
    "[LaTeX code]\n",
    "\n",
    "HMR is a difficult problem because it adds several features on top of normal handwriting recognition:\n",
    "\n",
    "1.   LaTeX is a computer language with a precise syntax, so minor 'grammatical' errors will cause the output to be unreadable.  Thus HMR models must learn the hidden syntax of '{''s and '_''s from just the images and their LaTeX labels.\n",
    "2.   The same symbol(s) can be scaled and translated in different ways.  So, if the label has learned that 'cos' should be mapped to '\\cos', then it must also learn this if 'cos' is written under a square root sign, or if is smaller and written above a fraction bar.\n",
    "\n",
    "To attack this difficult problem, Zhang, Du, and Dai [paper] have recently proposed a model they describe as multi-scale attention with dense encoder.  The exact architecture is described later, but we give the outline here.  The model is an encoder-decoder model with attention.  The encoder is a convolutional encoder based on the DenseNet architecture recently introduced by NAMES [densenet].  A DenseNet augments a normal convolutional neural network by feeding the convolutional layers the output of several preceeding layers, rather than just the immediate previous one.  [densenet] showed that this architecture performs excellently for image classification tasks, so [paper] adapts it to the problem of HMR.  The decoder is a one-directional GRU decoder with multi-scale attention; that is, it uses two context vectors for attention: a 'low-resolution' version for feature extraction and a 'high-resolution' version to extract small details.\n",
    "\n",
    "The Computer Recognition of Handwritten Mathematical Expressions (CROHME) is a recurring contest to solve the HMR problem.  [paper]'s model outperforms the state-of-the-art models from the 2014 and 2016 competitions.  On the 2016 dataset, for instance, [paper] acheived a 50.1% accuracy, higher than the first place model (49.6%), even though that model used a much larger training corpus pulled from Wikipedia.\n",
    "\n",
    "We implement the model described in [paper] and attempt to replicate it's success on a dataset from Kaggle [Kaggle] that combines several HMR datasets, including CROHME 2011.  We describe the data preprocessing (both here and in `preprocessing.ipynb`), detail the implementation of the architecture, train the model, and test its performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8TqVh8kNWNt"
   },
   "source": [
    "## Preliminary package loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vU7cEP3SN2-e"
   },
   "source": [
    "The following are the necessary packages for this code (note the `tf.enable_eager_execution()` flag).  This should be run on TensorFlow 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import helpers\n",
    "from inkml2img import inkml2img\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O-ftEDGt_fOP",
    "outputId": "504cbea8-a9fa-4a4d-d25a-eadfb6e719d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_VERSION = float('.'.join(tf.__version__.split('.')[:2]))\n",
    "TF_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5H1efPbGNgvK"
   },
   "source": [
    "This flag sets where the paths are going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqYXgJ684GBj"
   },
   "outputs": [],
   "source": [
    "# user = 'will'\n",
    "user = 'ben'\n",
    "is_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBqnY1FrUlqY"
   },
   "source": [
    "# Data preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples grabbed from: ['trainData_2012_part1', 'MatricesTrain2014', 'CROHME_training_2011', 'trainData_2012_part2', 'TrainINKML_2013']\n",
      "Test samples grabbed from: ['TestINKML_2013', 'MatricesTest2014', 'testData_2012', 'CROHME_test_2011']\n"
     ]
    }
   ],
   "source": [
    "# If not processing data, much of this code can be skipped\n",
    "PREPROCESSING = False\n",
    "TRAIN_X_PATH = 'train_X.p'\n",
    "TRAIN_Y_PATH = 'train_Y.p'\n",
    "\n",
    "if user == 'ben':\n",
    "    BASE_PATH = 'data'\n",
    "dirs = os.listdir(BASE_PATH)\n",
    "\n",
    "train_dirs = []\n",
    "test_dirs = []\n",
    "for d in dirs:\n",
    "    if 'train' in d.lower():\n",
    "        train_dirs.append(d)\n",
    "    if 'test' in d.lower():\n",
    "        test_dirs.append(d)\n",
    "        \n",
    "print('Training samples grabbed from:', train_dirs)\n",
    "print('Test samples grabbed from:', test_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 11350\n",
      "Test set files: 1629\n"
     ]
    }
   ],
   "source": [
    "# Create list of files in directory\n",
    "def get_file_list(directories, base_path='data/'):\n",
    "    files = []\n",
    "    for directory in directories:\n",
    "        path = os.path.join(BASE_PATH, directory)\n",
    "        files.extend([os.path.join(path, file) for file in os.listdir(path) if file.endswith(\".inkml\")])\n",
    "    return files\n",
    "\n",
    "if PREPROCESSING:\n",
    "    train_files = get_file_list(train_dirs)\n",
    "    test_files = get_file_list(test_dirs)\n",
    "    print('Total number of samples:', len(train_files))\n",
    "    print('Test set files:', len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute preprocessing\n",
    "\n",
    "### Code is set up to look for png's in an `images` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If images/ directory doesn't exist, create it:\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract labels, image paths, and any problem files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/new_plaid/mathhandwriting/inkml2img.py:224: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.axes().get_xaxis().set_visible(False)\n",
      "/anaconda3/envs/new_plaid/mathhandwriting/inkml2img.py:225: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.axes().get_yaxis().set_visible(False)\n",
      "/anaconda3/envs/new_plaid/mathhandwriting/inkml2img.py:226: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.axes().spines['top'].set_visible(False)\n",
      "/anaconda3/envs/new_plaid/mathhandwriting/inkml2img.py:227: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.axes().spines['right'].set_visible(False)\n",
      "/anaconda3/envs/new_plaid/mathhandwriting/inkml2img.py:228: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.axes().spines['bottom'].set_visible(False)\n",
      "/anaconda3/envs/new_plaid/mathhandwriting/inkml2img.py:229: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.axes().spines['left'].set_visible(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_Y, image_paths_X, problem_files_X = helpers.generate_dataset(train_files[:10])\n",
    "test_Y, image_paths_Y, problem_files_Y = helpers.generate_dataset(test_files[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process images to greyscale & uniform size, reshape for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREPROCESSING:\n",
    "    \"\"\"Process images, separate train & test sets, and save results\"\"\"\n",
    "    train_X = helpers.process_images(image_paths_X)\n",
    "    test_X = helpers.process_images(image_paths_Y)\n",
    "    train_X = np.asarray(train_X)\n",
    "    test_X = np.asarray(test_X)\n",
    "\n",
    "    train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], train_X.shape[2],1))\n",
    "    test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], test_X.shape[2],1))\n",
    "    train_Y = code_output(train_Y, max_targ_fmla, vocab)\n",
    "    \n",
    "    pickle.dump(train_X, open('train_X.p', 'wb'))\n",
    "    pickle.dump(train_Y, open('train_Y.p', 'wb'))\n",
    "    \n",
    "    SMALL_N = 100\n",
    "    pickle.dump(train_X[:SMALL_N], open('train_X_small.p', 'wb'))\n",
    "    pickle.dump(train_Y[:SMALL_N], open('train_Y_small.p', 'wb'))\n",
    "    \n",
    "else:\n",
    "    with open(TRAIN_X_PATH, 'rb') as p:\n",
    "        train_X = pickle.load(p)\n",
    "    with open(TRAIN_Y_PATH, 'rb') as p:\n",
    "        train_Y = pickle.load(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_{i} - x_{i + 1} + x_{i + 2}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check our labels are still in sync with our file paths\n",
    "plt.imshow(train_X[-1])\n",
    "print(train_Y[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that all images are uniform size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max dimensions of dataset: (255, 484)\n",
      "Number of images not the max dim size: 0\n"
     ]
    }
   ],
   "source": [
    "max_dims = helpers.get_max_dims(image_paths_X)\n",
    "print('Max dimensions of dataset:', max_dims)\n",
    "\n",
    "check = [True if x.shape == max_dims else False for x in train_X]\n",
    "\n",
    "# Is it all true?\n",
    "errors = sum(check) - len(check)\n",
    "print('Number of images not the max dim size:', errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the training array data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1fLrPTJOPUC"
   },
   "source": [
    "## Data loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8rV0hFvQrIi"
   },
   "source": [
    "The CROHME datasets are provided as inkML files tagged with the target value of the LaTeX code.  Another file--'Parsing inkml.ipynb'--explains the file extension and the conversion from the original files to a list of dictionaries giving a 200x200 image array and the LaTeX string.\n",
    "\n",
    "Provide the location of 'savedimgs.p' as PATH, then we load it to 'content.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "rUZ0pRROUsD-",
    "outputId": "c7afed0b-ee70-409d-e3a0-36bd38db3491"
   },
   "outputs": [],
   "source": [
    "if is_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZI8kD47YSBpu"
   },
   "source": [
    "In preprocessing we separated training and test sets per CROHME designation. Our training data is in `train_X` and their labels, latex strings, are found in `train_Y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOGuL_9Lyv-4"
   },
   "outputs": [],
   "source": [
    "PATH = '/content/gdrive/My Drive/Colab Notebooks/MHR Final/train_X_chunk0.p'\n",
    "RELOAD_DATA = False\n",
    "\n",
    "if RELOAD_DATA:\n",
    "    with open(PATH, 'rb') as pickle_file:\n",
    "        train_X = pickle.load(pickle_file)\n",
    "\n",
    "    PATH = '/content/gdrive/My Drive/Colab Notebooks/MHR Final/train_labels.p'\n",
    "\n",
    "    with open(PATH, 'rb') as pickle_file:\n",
    "        train_Y = pickle.load(pickle_file)\n",
    "    \n",
    "    # Be sure to subset the labels if you're subsetting the training data\n",
    "    Y = Y[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "zHG8s1fPWSJk",
    "outputId": "69b60786-e1e0-4da6-a963-96a5c17155e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0 \\times 2 9 x^{2 8}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(len(train_X))\n",
    "\n",
    "plt.imshow(train_X[i])\n",
    "print(train_Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yFxzPK0NyVLi",
    "outputId": "f5858b38-c601-4511-cecf-29cd563e97c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 484)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1A-6MuKYcSn"
   },
   "source": [
    "## Tokenizing, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0A7lYOWTPoO"
   },
   "source": [
    "Here we process the labels, `Y`.  The lables are provided as strings with some additional whitespace (LaTeX's ubiquitous \\$ are omitted throughout). `tokenize` converts such a string to a list of symbols with no whitespace and with '<start>' and '<end>' tokens added.  Care must be taken because LaTeX uses \\\\ to start multicharacter commands (\\\\frac), and we want to code these as a single symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdyN134KYkVY"
   },
   "outputs": [],
   "source": [
    "def tokenize(formula):\n",
    "    # takes a string of LaTeX code and breaks it into its consituent pieces\n",
    "    # also removes spaces\n",
    "    # adds '<end>' to the end and '<start>' to the start\n",
    "    as_list = ['<start>']\n",
    "    multi = False\n",
    "    first = False\n",
    "    temp = ''\n",
    "    esc_chars = ['_', '^', ' ', '{', '}', ',', '(', ')', '+', '-', '*', '\\\\','[', ']', '&', '/', '0','1','2','3','4','5','6','7','8','9']\n",
    "    weak_esc_chars = ['{','}','(',')','[',']']\n",
    "    for char in formula:\n",
    "        if not multi and char != '\\\\' and char != ' ':\n",
    "            as_list.append(char)\n",
    "        if multi and not first:\n",
    "            if char in esc_chars:\n",
    "                as_list.append(temp)\n",
    "                multi = False\n",
    "                temp = ''\n",
    "                if char != ' ':\n",
    "                    as_list.append(char)\n",
    "            else:\n",
    "                temp = temp + char\n",
    "        if multi and first:\n",
    "            if char in weak_esc_chars:\n",
    "                as_list.append(temp)\n",
    "                multi = False\n",
    "                temp = ''\n",
    "                if char != ' ':\n",
    "                    as_list.append(char)\n",
    "            else:\n",
    "                temp = temp + char\n",
    "        if char == '\\\\':\n",
    "            temp = char\n",
    "            multi = True\n",
    "            first = True\n",
    "    as_list.append('<end>')\n",
    "    return as_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JnplB-6PPh4v"
   },
   "source": [
    "`get_vocab` returns all of the symbols used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "heVdKjehYxjc"
   },
   "outputs": [],
   "source": [
    "def get_vocab(formulas):\n",
    "    # returns the LaTeX symbols that are used and always puts '<end>' at the start\n",
    "    vocab = ['PAD', '<end>', '<start>']\n",
    "    for formula in formulas:\n",
    "        for word in tokenize(formula):\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "    vocab.sort()\n",
    "    vocab.insert(0, vocab.pop(vocab.index('<start>')))\n",
    "    vocab.insert(0, vocab.pop(vocab.index('<end>')))\n",
    "    vocab.insert(0, vocab.pop(vocab.index('PAD')))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6U3Y7O5cZXQG"
   },
   "outputs": [],
   "source": [
    "vocab = get_vocab(train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7TBoBFjPp7U"
   },
   "source": [
    "This allows us to encode symbols as single numbers.  We can use `vocab[int]` to give the symbol corresponding to `int` and `vocab.index(str)` to give the number corresponding to the symbol `str`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "D8zUQ0DiZSAj",
    "outputId": "8dc9b8ed-6957-4d27-d19a-be183b4efcb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "['\\\\sqrt', '\\\\sqrt', 'PAD', '3', '+']\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print([vocab[i] for i in np.random.randint(len(vocab), size=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRxipy52ZuQq"
   },
   "outputs": [],
   "source": [
    "def max_length_targ(formulas):\n",
    "    max_len = 0\n",
    "    for formula in formulas:\n",
    "        max_len = max(max_len, len(tokenize(formula)))\n",
    "    return max_len\n",
    "\n",
    "max_targ_fmla = max_length_targ(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_WUB3Zhcbw2W",
    "outputId": "5a736764-6eac-4e42-f33e-3a7d6171b074"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_targ_fmla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tov_KkHcQWkF"
   },
   "source": [
    "`encode_latex` turns a formula string into a list of numbers (of constant length when `pad = True`) and `decode_latex` turns a list of numbers into the corresponding LaTeX string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFD1JqSlcB0U"
   },
   "outputs": [],
   "source": [
    "def encode_latex(string, vocab, pad = False, length=0):\n",
    "    # Turns a LaTeX string into a encoded vector based on vocab ordering\n",
    "    vector = [vocab.index(char) for char in tokenize(string)]\n",
    "    if pad:\n",
    "        while len(vector) < length:\n",
    "            vector.append(0)\n",
    "    return np.asarray(vector)\n",
    "\n",
    "def decode_latex(vector, vocab,pad=False):\n",
    "    string = ''\n",
    "    vector = list(vector)\n",
    "    for num in vector:\n",
    "        string = string + vocab[num]\n",
    "        if num == 0:\n",
    "            break\n",
    "    return string\n",
    "\n",
    "def code_output(formulas, fmla_length, vocab):\n",
    "    # assumes that the formulas come in as un tokenized strings\n",
    "    coded_formulas = [encode_latex(formula,vocab, pad = True,length=fmla_length) for formula in formulas]\n",
    "    return np.asarray(coded_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7JMfBNvkcdag",
    "outputId": "b11e62da-3b30-4c4c-9dbd-c85e6d7211f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 255, 484, 1) (10, 37)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape, train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "GDkrK25324Mi",
    "outputId": "2cc15ca0-2d28-422d-aadf-fde7c36fc352"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8af89c2898>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_X[2,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPowC_J-cmkJ"
   },
   "source": [
    "## As tf.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6ndzPWLcopU"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_X)\n",
    "BATCH_SIZE = 8\n",
    "N_BATCH = BUFFER_SIZE // BATCH_SIZE\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_X, train_Y)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ieNUvETsbH5"
   },
   "source": [
    "# DenseNet Encoder with Multi-Scale Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69YX7GjfPUKY"
   },
   "source": [
    "We implement a model based of [paper], a DenseNet Encoder with Multi-Scale Attention.  There are two main novel features to this model that we want to explore:\n",
    "\n",
    "\n",
    "1.   The use of DenseNets (with Bottlenecks) in the encoder\n",
    "2.   The use of multi-scale attention in the decoder\n",
    "\n",
    "We explain the idea behind these notions in the relevant section.  The implementation is based on a combination of of [K],[nmt],[paper].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FH7y4Ak3rseK"
   },
   "source": [
    "## Prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VG1is5qRZ5Zu"
   },
   "source": [
    "We set several global variables to use in building and training the model.  These will be explained when used.  CP_PATH is the file path for checkpoints of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFc5PJUo_BJa"
   },
   "outputs": [],
   "source": [
    "### Some parameters\n",
    "\n",
    "if user == 'will':\n",
    "    CP_PATH = '/content/gdrive/My Drive/Colab Notebooks/MHR Final/training_checkpoints/DNB_Bahd'\n",
    "if user == 'ben':\n",
    "    CP_PATH = 'checkpoints/'\n",
    "\n",
    "TOTAL_BLOCKS = 3\n",
    "GROWTH_RATE = 24 # denoted k in paper\n",
    "DEPTH = 32 # denoted D in paper\n",
    "DROPOUT = 0.2 # fraction to dropout\n",
    "\n",
    "ATTENTION_GROWTH_RATE = 24\n",
    "ATTENTION_DEPTH = 16\n",
    "\n",
    "EMBEDDING_DIM = 32 # 256 in paper, reduced due to size of dataset\n",
    "GRU_UNITS = 128 # 256 in paper, reduced due to size of dataset\n",
    "\n",
    "\n",
    "## From the dataset\n",
    "INPUT_SHAPE = train_X[0].shape[1:]\n",
    "IS_TRAINING = True\n",
    "OUTPUT_SIZE = len(vocab) # number of symbols in LaTeX vocabulary\n",
    "\n",
    "MAX_LENGTH_TARG = max_targ_fmla # How long we want to run the predictor until we force it to end\n",
    "\n",
    "TARG_START_TOKEN = '<start>'\n",
    "TARG_END_TOKEN = '<end>'\n",
    "\n",
    "\n",
    "\n",
    "## Fine-tuning the model\n",
    "\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1dW2yd_aC13"
   },
   "source": [
    "We also use the gru wrapper from [nmt] to automatically detect a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74V9nUMaZ4r-"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    if tf.test.is_gpu_available(): \n",
    "  # if gpu: \n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeXr9cOFrxL1"
   },
   "source": [
    "## Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_P-lDXz5ajUe"
   },
   "source": [
    "### DenseNet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58HkIEW1ai55"
   },
   "source": [
    "\n",
    "We build the DenseNet encoder from [paper] in Keras.  The DenseNet architecture was developed in [DenseNet], and our implementation owes much to the TensorFlow implementation developed by K-- [K].  The overall architecture of the encoder is:\n",
    "\n",
    "\n",
    "1.   Image input\n",
    "2.   Initial 7x7 convolution with 2x2 and 2x2 max-pooling\n",
    "3.   DenseNetB block\n",
    "4.   Transition layer (convolution and average-pool)\n",
    "5.   DenseNetB block\n",
    "6.   Transition layer\n",
    "7.   DenseNetB block\n",
    "\n",
    "\n",
    "\n",
    "The main feature of the encoder are the DenseNet blocks, which take in parameters $k$ for growth rate and $D$ for depth.  The blocks are labeled DenseNetB because they include bottlenecks.\n",
    "\n",
    "Each DenseNetB block is made up of several 3x3 convolutional layers (with normalization/activation/etc.), with a twist.  Typically, convlutional layers are done in sequence, with the input to each layer being the output to the previous layer:\n",
    "\n",
    "$L_0 = \\text{Conv}(\\text{input})$\n",
    "\n",
    "$L_1 = \\text{Conv}(L_0)$\n",
    "\n",
    "$L_2 = \\text{Conv}(L_1)$\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "However, in DenseNetB block, the first layer remains the same:\n",
    "\n",
    "$L^{DN}_0 = \\text{Conv}(\\text{input})$\n",
    "\n",
    "Then, each additional layer takes as input all convolutional outputs concatenated together:\n",
    "\n",
    "$L^{DN}_1 = \\text{Conv}(L^{DN}_0)$\n",
    "\n",
    "$L^{DN}_2 = \\text{Conv}(L^{DN}_0, L^{DN}_1)$\n",
    "\n",
    "$L^{DN}_3 = \\text{Conv}(L^{DN}_0, L^{DN}_1, L^{DN}_2)$\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "This does XXXX\n",
    "\n",
    "Between each DenseNetB block, a transition layer is added consising of a 1x1 convolutional layer and a 2x2 average pooling layer.  This cuts the number of channels in half and reduces dimensionality to keep the size of thr layers under control.\n",
    "\n",
    "Up until now, we have described the structure of the original DenseNet block.  The bottleneck layer were a layer addition that improves computational efficiency by reducing the number of channels.  The bottleneck layer is similar in structure to a layer of DenseNet, but the convolutional layer is 1x1 and reduces the number of filters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTH64QnRaMbI"
   },
   "source": [
    "### Multi-scale Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PsrDDoXurkz"
   },
   "source": [
    "The decoder will be a single GRU layer.  This means that it will take in a context vector.  However, just taking the output of the DNB leads to resolution errors.  That is, the DenseNet output ends up being squashed to a low-resolution context vector.  This is fine for some things, but in mathematical formulas, there are small features--small enough to be confused for noise--that end up greatly affecting the meaning of the expression.  A perfect example (provided in [paper]) is the decimal point in '3.000003.'  A low-resoluion context vector might completely miss the '.', leading to massive change in the intended meaning.  \n",
    "\n",
    "To fix this, [paper] employs multi-scale attention by forking another path off from the middle of the transition layer 6 (before pooling occurs).  This output is then put through another DenseNet block (although with differed depth) to provide a more high-resolution context.  The two context vectors are then separately fed into an attention mechanism, and the results are concatenated (see the implementation of the DNBDecoder).    This high resolution image will allow the model to spot the import decimal point in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGcsTN7K7kP0"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfIAMs_q8iEj"
   },
   "source": [
    "We now work through the specifics of the parameter calls.  The parameters are given default values (from [paper] or even [densenet])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U88iLBDm-gAV"
   },
   "outputs": [],
   "source": [
    "class DNBEncoder(tf.keras.Model):\n",
    "    ## implements the DenseNet with Bottlenecks encoder from paper\n",
    "    ## Update to actual variables we want\n",
    "    def __init__(self,\n",
    "                 inp_shape = INPUT_SHAPE,\n",
    "                 dropout_frac = DROPOUT,\n",
    "                 total_blocks = TOTAL_BLOCKS,\n",
    "                 depth = DEPTH,\n",
    "                 attention_growth_rate = ATTENTION_GROWTH_RATE,\n",
    "                 attention_depth = ATTENTION_DEPTH,\n",
    "                 growth_rate = GROWTH_RATE,\n",
    "                 enc_units = GRU_UNITS,\n",
    "                 batch_size = BATCH_SIZE):\n",
    "        super(DNBEncoder, self).__init__()\n",
    "        self.inp_shape = inp_shape\n",
    "        self.dropout_frac = dropout_frac\n",
    "        self.total_blocks = total_blocks\n",
    "        self.depth = depth\n",
    "        self.attention_growth_rate = attention_growth_rate\n",
    "        self.attention_depth = attention_depth\n",
    "        self.growth_rate = growth_rate\n",
    "        self.enc_units = GRU_UNITS\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        # New variables\n",
    "        self.gru = gru(enc_units)\n",
    "                \n",
    "    def call(self, inp, hidden, is_training):\n",
    "        bias = False\n",
    "        \n",
    "        # Initial Conv2D layer: kernel reads 7 x 7, strides are 2 x 2, and the number of output channels is 48\n",
    "        x = tf.keras.layers.Conv2D(48, (7,7), strides = (2,2), input_shape = self.inp_shape, use_bias = bias)(inp)\n",
    "        \n",
    "        \n",
    "        # Feed into a MaxPooling layer\n",
    "        x = self.add_maxpool(x, k=2)\n",
    "        \n",
    "        for block in range(self.total_blocks):\n",
    "            if block > 0:\n",
    "            # This puts a transition layer between each of the blocks\n",
    "                x = self.add_conv2d(x, spot_size=1, num_output_channels = int(self.growth_rate//2))\n",
    "                if block == self.total_blocks - 1:\n",
    "                # Peels off in the middle of the final transition layer to have a high-resolution context vector\n",
    "                    y = self.add_DNB(x, self.attention_growth_rate, is_training, bias, depth=self.attention_depth)\n",
    "                x = self.add_avgpool(x, k=2)\n",
    "            # Adds the DenseNetB block\n",
    "            x = self.add_DNB(x, self.growth_rate, is_training, bias, depth=self.depth)\n",
    "            \n",
    "        \n",
    "        # Reshapes the outputs to feed into the GRU decoder, turning (-,a,b, c) into (-, a*b, c)\n",
    "        #x = tf.keras.layers.Reshape((144, 36))(x)\n",
    "        #y = tf.keras.layers.Reshape((576,36))(y)\n",
    "        x = tf.keras.layers.Reshape((x.shape[1]*x.shape[2],x.shape[3]))(x)\n",
    "        y = tf.keras.layers.Reshape((y.shape[1]*y.shape[2],y.shape[3]))(y)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def add_conv2d(self, inp, spot_size, num_output_channels, stride = 1, bias = True, padding = 'SAME'):\n",
    "        # Adds a 2D convolutional layer with a spotlight size of spot_size x spot_size and a stride of stride x stride that has num_output_channels many output features\n",
    "        x = tf.keras.layers.Conv2D(num_output_channels, kernel_size = spot_size, strides = stride, use_bias = bias, padding = padding)(inp)\n",
    "        return x\n",
    "                \n",
    "    def add_maxpool(self, inp, k):\n",
    "        x = tf.keras.layers.MaxPool2D(pool_size=k)(inp)\n",
    "        return x\n",
    "        \n",
    "    def add_avgpool(self, inp, k):\n",
    "        x = tf.keras.layers.AveragePooling2D(pool_size=k)(inp)\n",
    "        return x\n",
    "        \n",
    "    def add_DNB(self, inp, growth_rate, is_training, bias, depth):\n",
    "        x = inp\n",
    "        layers_per_block = (depth - (self.total_blocks + 1)) // self.total_blocks\n",
    "        for layer in range(layers_per_block):\n",
    "            x = self.add_bottleneck(inp, growth_rate, is_training, bias=bias, padding='VALID')\n",
    "            x = self.add_DN(x, output_features = growth_rate, kernel_size = 3, is_training = is_training, bias=bias)\n",
    "            x = tf.concat(axis=3, values=(inp, x))\n",
    "        return x\n",
    "    \n",
    "    def add_bottleneck(self, inp, growth_rate, is_training, bias, padding='VALID'):\n",
    "        x = tf.keras.layers.BatchNormalization()(inp, is_training)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.add_conv2d(x, num_output_channels = 4 * growth_rate, spot_size = 1, bias=bias, padding=padding)\n",
    "        x = self.add_dropout(x, is_training)\n",
    "        return(x)\n",
    "    \n",
    "    def add_dropout(self, inp, is_training):\n",
    "        if self.dropout_frac < 1:\n",
    "            x = tf.keras.layers.Dropout(self.dropout_frac)(inp, is_training)\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def add_DN(self, inp, output_features, kernel_size = 3, bias=False, is_training = True):\n",
    "        x = self.add_comp(inp, output_features, kernel_size = kernel_size, bias=bias, is_training=is_training)\n",
    "        return x\n",
    "    \n",
    "    def add_comp(self, inp, out_features, kernel_size=3, bias=False, is_training = True):\n",
    "        x = tf.keras.layers.BatchNormalization()(inp, is_training)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.add_conv2d(x, num_output_channels = out_features, spot_size = kernel_size, bias = bias)\n",
    "        x = self.add_dropout(x, is_training)\n",
    "        return x        \n",
    "            \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPUBEwtfr43N"
   },
   "source": [
    "## Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yAjEpebj7cBw"
   },
   "source": [
    "The decoder is, at its base, a standard GRU encoder.  This will allow us to predict the next symbol in the formula based on the encoder output and the previous (predicted symbol).  It is augmented by adding the multi-scale attention as mentioned above.  Calling the decoder includes providing the low-resolution and high-resolution outputs of the encoder.  These are then put through a Bahdanau attention layer separately, and then concatenated with the previous value to be put through the GRU decoder.\n",
    "\n",
    "We return the attention weights even though we do nothing with them.  Future work could implement an attention grapher that displays where the model is paying attention at each symbol (the paper implements such a graph).\n",
    "\n",
    "[Insert Fig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxnDNGBEl7gx"
   },
   "outputs": [],
   "source": [
    "class DNBDecoder(tf.keras.Model):\n",
    "    def __init__(self, target_language_size=OUTPUT_SIZE, embedding_dim=EMBEDDING_DIM, dec_units=GRU_UNITS, batch_size = BATCH_SIZE):\n",
    "        super(DNBDecoder, self).__init__()\n",
    "        self.target_language_size = target_language_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.batch_size = batch_size\n",
    "        # New variables\n",
    "        self.embedding = tf.keras.layers.Embedding(self.target_language_size, self.embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(self.target_language_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1_low = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2_low = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V_low = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        self.W1_high = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2_high = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V_high = tf.keras.layers.Dense(1)\n",
    "\n",
    "        \n",
    "    def call(self, inp, hidden, low_output, high_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score_low = self.V_low(tf.nn.tanh(self.W1_low(low_output) + self.W2_low(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights_low = tf.nn.softmax(score_low, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector_low = attention_weights_low * low_output\n",
    "        context_vector_low = tf.reduce_sum(context_vector_low, axis=1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score_high = self.V_high(tf.nn.tanh(self.W1_high(high_output) + self.W2_high(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights_high = tf.nn.softmax(score_high, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector_high = attention_weights_high * high_output\n",
    "        context_vector_high = tf.reduce_sum(context_vector_high, axis=1)\n",
    "\n",
    "        context_vector = tf.concat([context_vector_low, context_vector_high], axis=1)\n",
    "        \n",
    "        #print('inp:',inp)\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(inp)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        #print('emb:',x)\n",
    "        #print('context:',context_vector)\n",
    "        #print('expand:', tf.expand_dims(context_vector,1))\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        #x = tf.concat([context_vector, x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights_low, attention_weights_high#, context_vector_low, context_vector_high\n",
    "\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.dec_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-9bqaeSsAcB"
   },
   "source": [
    "## Epilogue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmBg6I_QWW74"
   },
   "source": [
    "We add in the optimizers and loss.  We follow [paper] and use Adadelta optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xSJ0-97EWWrW"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdadeltaOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pPae_U_7WV6f"
   },
   "source": [
    "Here we instantiate the encoder and decoder.  Everything runs off default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCPX_709iSPG"
   },
   "outputs": [],
   "source": [
    "encoder = DNBEncoder()\n",
    "decoder = DNBDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L2VbRd6k2You",
    "outputId": "bd88d460-13a5-4fd2-9d0f-0af01b714b97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inp_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tnx-T3ivg1Pu"
   },
   "source": [
    "Now, let's add checkpointing.  This will save checkpoints to the folder CP_PATH specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xZ7m-7Ig310"
   },
   "outputs": [],
   "source": [
    "if user == 'ben':\n",
    "    if not os.path.exists(CP_PATH):\n",
    "        os.makedirs(CP_PATH)\n",
    "checkpoint_prefix = os.path.join(CP_PATH, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tiH4Pn3hicEC"
   },
   "source": [
    "# Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQJKpRNMDWXa"
   },
   "source": [
    "The following is the training routine that runs for EPOCHS many epochs.  We use teacher forcing and base the algorithm on the one in [nmt].  Note that there is no GRU in the encoder, so it produces no hidden state.  Instead, we use an initialized hidden state to start the decoder, and keep feeding it back to itself (as in [paper])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "Vi7sguPxisdi",
    "outputId": "684dd76f-92e5-4f9c-b716-84b4aa24f88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1 Batch 0 Loss 1.9305\n",
      "Epoch 1 Loss 1.9305\n",
      "Time taken for 1 epoch 3.805291175842285 sec\n",
      "\n",
      "Estimated seconds to completion: 34.24771070480347\n",
      "0\n",
      "Epoch 2 Batch 0 Loss 1.8711\n",
      "Epoch 2 Loss 1.8711\n",
      "Time taken for 1 epoch 3.739017963409424 sec\n",
      "\n",
      "Estimated seconds to completion: 29.912256240844727\n",
      "0\n",
      "Epoch 3 Batch 0 Loss 1.9039\n",
      "Epoch 3 Loss 1.9039\n",
      "Time taken for 1 epoch 3.632991075515747 sec\n",
      "\n",
      "Estimated seconds to completion: 25.4310142993927\n",
      "0\n",
      "Epoch 4 Batch 0 Loss 1.8777\n",
      "Epoch 4 Loss 1.8777\n",
      "Time taken for 1 epoch 3.7730679512023926 sec\n",
      "\n",
      "Estimated seconds to completion: 22.638563632965088\n",
      "0\n",
      "Epoch 5 Batch 0 Loss 1.9346\n",
      "Epoch 5 Loss 1.9346\n",
      "Time taken for 1 epoch 3.8128578662872314 sec\n",
      "\n",
      "Estimated seconds to completion: 19.064350128173828\n",
      "0\n",
      "Epoch 6 Batch 0 Loss 1.9265\n",
      "Epoch 6 Loss 1.9265\n",
      "Time taken for 1 epoch 3.7757511138916016 sec\n",
      "\n",
      "Estimated seconds to completion: 15.103059768676758\n",
      "0\n",
      "Epoch 7 Batch 0 Loss 1.9064\n",
      "Epoch 7 Loss 1.9064\n",
      "Time taken for 1 epoch 3.8414647579193115 sec\n",
      "\n",
      "Estimated seconds to completion: 11.524442911148071\n",
      "0\n",
      "Epoch 8 Batch 0 Loss 1.9015\n",
      "Epoch 8 Loss 1.9015\n",
      "Time taken for 1 epoch 3.7205488681793213 sec\n",
      "\n",
      "Estimated seconds to completion: 7.4411301612854\n",
      "0\n",
      "Epoch 9 Batch 0 Loss 1.9113\n",
      "Epoch 9 Loss 1.9113\n",
      "Time taken for 1 epoch 3.831439256668091 sec\n",
      "\n",
      "Estimated seconds to completion: 3.831529140472412\n",
      "0\n",
      "Epoch 10 Batch 0 Loss 1.8961\n",
      "Epoch 10 Loss 1.8961\n",
      "Time taken for 1 epoch 3.8170008659362793 sec\n",
      "\n",
      "Estimated seconds to completion: 0.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            print(batch)\n",
    "            enc_output_low, enc_output_high = encoder(inp, hidden, is_training=True)\n",
    "            dec_hidden = hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([vocab.index('<start>')] * BATCH_SIZE, 1)       \n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, dec_hidden, _, _ = decoder(dec_input, dec_hidden, enc_output_low, enc_output_high)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    print('Estimated seconds to completion: {}'.format((EPOCHS-epoch-1)*(time.time()-start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-J1PkwKunHdS"
   },
   "source": [
    "# Evaluating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEG-YAFrElfa"
   },
   "source": [
    "First, let's reload the last checkpoint in case you don't want to rerun the training everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJrU1tvNE53i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f8adc953908>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e-AaQ4R3E6hc"
   },
   "source": [
    "The following functions give the LaTeX predictions for an input image by feeding the image into the encoder, and then seeing what the decoder decodes.  It is important to set `is_training = False` to tell the BatchNormalization and Dropout layers that this is a prediction run, not a training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4AfgTvlnd59"
   },
   "outputs": [],
   "source": [
    "def evaluate(image, encoder, decoder, latex_vocab=vocab, max_length_targ = MAX_LENGTH_TARG):\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = tf.zeros((BATCH_SIZE, GRU_UNITS))\n",
    "\n",
    "    enc_out_low, enc_out_high = encoder(image, hidden, is_training=False)\n",
    "\n",
    "    dec_hidden = hidden\n",
    "    dec_input = tf.expand_dims([vocab.index('<start>')] * BATCH_SIZE, 1)       \n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, _, _ = decoder(dec_input, dec_hidden, enc_out_low, enc_out_high)#, is_training=False)\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += latex_vocab[predicted_id]\n",
    "\n",
    "        if latex_vocab[predicted_id] == '<end>':\n",
    "            return result, image\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id] * BATCH_SIZE,1)\n",
    "        \n",
    "    return result, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_j09JoTplqM"
   },
   "outputs": [],
   "source": [
    "def texify(image, encoder = encoder, decoder = decoder, latex_vocab=vocab, max_length_targ = MAX_LENGTH_TARG):\n",
    "    result,_ = evaluate(np.reshape(image,(1,max_dims[0],max_dims[1],1)), encoder, decoder, latex_vocab, max_length_targ)\n",
    "    print(result)\n",
    "    print(\"should represent\")\n",
    "    plt.imshow(image[:,:,0])#display_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Onm5vPI7TzOB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\\log_\n",
      "should represent\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = train_X[np.random.randint(len(train_X))]\n",
    "texify(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jN73QOAhsMRK"
   },
   "source": [
    "### Validating\n",
    "\n",
    "We should have split the data set into test and train, so let's evaluate the appropriate metric on our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igonykajsmyC"
   },
   "source": [
    "# Epilogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9XHE7azIspX0"
   },
   "source": [
    "In which we draw conclusions about how we deserve an A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jX35x2AOjOn"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hw3sXICpOlk4"
   },
   "source": [
    "\n",
    "\n",
    "1.   Paper we implement: https://arxiv.org/abs/1801.03530\n",
    "2.   DenseNet original: https://arxiv.org/abs/1608.06993\n",
    "3.   Khlestov implementation: [here](https://github.com/ikhlestov/vision_networks) for the github, and [here](https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#.55qu3tfqm) for more commentary\n",
    "4.    CROHME data set:\n",
    "5.    nmt_with_attention:\n",
    "6.    nmt_with_attention(class_version):\n",
    "7.    Bahdanau attention:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3L7L48iiBXyf"
   },
   "source": [
    "# Old stuff and Notes\n",
    "\n",
    "## TensorFlow implementation\n",
    "\n",
    "The following is/is based on the TensorFlow implementation by Illarion Khlestov.  See [here](https://github.com/ikhlestov/vision_networks) for the github, and [here](https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#.55qu3tfqm) for more commentary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edYNf8SmB-OU"
   },
   "source": [
    "This has the following requirements: ipdb\n",
    "ipython\n",
    "matplotlib\n",
    "numpy\n",
    "Pillow\n",
    "scipy\n",
    "\n",
    "and either tensorflow>=0.10.0\n",
    " or tensorflow-gpu>=0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "92yLkEr3CeWd"
   },
   "source": [
    "The following is the building of DenseNet (models.dense_net in the main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cwdycRtB4qq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "TF_VERSION = float('.'.join(tf.__version__.split('.')[:2]))\n",
    "\n",
    "\n",
    "class DenseNet:\n",
    "    def __init__(self, data_provider, growth_rate, depth,\n",
    "                 total_blocks, keep_prob, num_inter_threads, num_intra_threads,\n",
    "                 weight_decay, nesterov_momentum, model_type, dataset,\n",
    "                 should_save_logs, should_save_model,\n",
    "                 renew_logs=False,\n",
    "                 reduction=1.0,\n",
    "                 bc_mode=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Class to implement networks from this paper\n",
    "        https://arxiv.org/pdf/1611.05552.pdf\n",
    "        Args:\n",
    "            data_provider: Class, that have all required data sets\n",
    "            growth_rate: `int`, variable from paper\n",
    "            depth: `int`, variable from paper\n",
    "            total_blocks: `int`, paper value == 3\n",
    "            keep_prob: `float`, keep probability for dropout. If keep_prob = 1\n",
    "                dropout will be disables\n",
    "            weight_decay: `float`, weight decay for L2 loss, paper = 1e-4\n",
    "            nesterov_momentum: `float`, momentum for Nesterov optimizer\n",
    "            model_type: `str`, 'DenseNet' or 'DenseNet-BC'. Should model use\n",
    "                bottle neck connections or not.\n",
    "            dataset: `str`, dataset name\n",
    "            should_save_logs: `bool`, should logs be saved or not\n",
    "            should_save_model: `bool`, should model be saved or not\n",
    "            renew_logs: `bool`, remove previous logs for current model\n",
    "            reduction: `float`, reduction Theta at transition layer for\n",
    "                DenseNets with bottleneck layers. See paragraph 'Compression'\n",
    "                https://arxiv.org/pdf/1608.06993v3.pdf#4\n",
    "            bc_mode: `bool`, should we use bottleneck layers and features\n",
    "                reduction or not.\n",
    "        \"\"\"\n",
    "        self.data_provider = data_provider\n",
    "        self.data_shape = data_provider.data_shape\n",
    "        self.n_classes = data_provider.n_classes\n",
    "        self.depth = depth\n",
    "        self.growth_rate = growth_rate\n",
    "        self.num_inter_threads = num_inter_threads\n",
    "        self.num_intra_threads = num_intra_threads\n",
    "        # how many features will be received after first convolution\n",
    "        # value the same as in the original Torch code\n",
    "        self.first_output_features = growth_rate * 2\n",
    "        self.total_blocks = total_blocks\n",
    "        self.layers_per_block = (depth - (total_blocks + 1)) // total_blocks\n",
    "        self.bc_mode = bc_mode\n",
    "        # compression rate at the transition layers\n",
    "        self.reduction = reduction\n",
    "        if not bc_mode:\n",
    "            print(\"Build %s model with %d blocks, \"\n",
    "                  \"%d composite layers each.\" % (\n",
    "                      model_type, self.total_blocks, self.layers_per_block))\n",
    "        if bc_mode:\n",
    "            self.layers_per_block = self.layers_per_block // 2\n",
    "            print(\"Build %s model with %d blocks, \"\n",
    "                  \"%d bottleneck layers and %d composite layers each.\" % (\n",
    "                      model_type, self.total_blocks, self.layers_per_block,\n",
    "                      self.layers_per_block))\n",
    "        print(\"Reduction at transition layers: %.1f\" % self.reduction)\n",
    "\n",
    "        self.keep_prob = keep_prob\n",
    "        self.weight_decay = weight_decay\n",
    "        self.nesterov_momentum = nesterov_momentum\n",
    "        self.model_type = model_type\n",
    "        self.dataset_name = dataset\n",
    "        self.should_save_logs = should_save_logs\n",
    "        self.should_save_model = should_save_model\n",
    "        self.renew_logs = renew_logs\n",
    "        self.batches_step = 0\n",
    "\n",
    "        self._define_inputs()\n",
    "        self._build_graph()\n",
    "        self._initialize_session()\n",
    "        self._count_trainable_params()\n",
    "\n",
    "    def _initialize_session(self):\n",
    "        \"\"\"Initialize session, variables, saver\"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "\n",
    "        # Specify the CPU inter and Intra threads used by MKL\n",
    "        config.intra_op_parallelism_threads = self.num_intra_threads\n",
    "        config.inter_op_parallelism_threads = self.num_inter_threads\n",
    "\n",
    "        # restrict model GPU memory utilization to min required\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        tf_ver = int(tf.__version__.split('.')[1])\n",
    "        if TF_VERSION <= 0.10:\n",
    "            self.sess.run(tf.initialize_all_variables())\n",
    "            logswriter = tf.train.SummaryWriter\n",
    "        else:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            logswriter = tf.summary.FileWriter\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.summary_writer = logswriter(self.logs_path)\n",
    "\n",
    "    def _count_trainable_params(self):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "            shape = variable.get_shape()\n",
    "            variable_parametes = 1\n",
    "            for dim in shape:\n",
    "                variable_parametes *= dim.value\n",
    "            total_parameters += variable_parametes\n",
    "        print(\"Total training params: %.1fM\" % (total_parameters / 1e6))\n",
    "\n",
    "    @property\n",
    "    def save_path(self):\n",
    "        try:\n",
    "            save_path = self._save_path\n",
    "        except AttributeError:\n",
    "            save_path = 'saves/%s' % self.model_identifier\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            save_path = os.path.join(save_path, 'model.chkpt')\n",
    "            self._save_path = save_path\n",
    "        return save_path\n",
    "\n",
    "    @property\n",
    "    def logs_path(self):\n",
    "        try:\n",
    "            logs_path = self._logs_path\n",
    "        except AttributeError:\n",
    "            logs_path = 'logs/%s' % self.model_identifier\n",
    "            if self.renew_logs:\n",
    "                shutil.rmtree(logs_path, ignore_errors=True)\n",
    "            os.makedirs(logs_path, exist_ok=True)\n",
    "            self._logs_path = logs_path\n",
    "        return logs_path\n",
    "\n",
    "    @property\n",
    "    def model_identifier(self):\n",
    "        return \"{}_growth_rate={}_depth={}_dataset_{}\".format(\n",
    "            self.model_type, self.growth_rate, self.depth, self.dataset_name)\n",
    "\n",
    "    def save_model(self, global_step=None):\n",
    "        self.saver.save(self.sess, self.save_path, global_step=global_step)\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.saver.restore(self.sess, self.save_path)\n",
    "        except Exception as e:\n",
    "            raise IOError(\"Failed to to load model \"\n",
    "                          \"from save path: %s\" % self.save_path)\n",
    "        self.saver.restore(self.sess, self.save_path)\n",
    "        print(\"Successfully load model from save path: %s\" % self.save_path)\n",
    "\n",
    "    def log_loss_accuracy(self, loss, accuracy, epoch, prefix,\n",
    "                          should_print=True):\n",
    "        if should_print:\n",
    "            print(\"mean cross_entropy: %f, mean accuracy: %f\" % (\n",
    "                loss, accuracy))\n",
    "        summary = tf.Summary(value=[\n",
    "            tf.Summary.Value(\n",
    "                tag='loss_%s' % prefix, simple_value=float(loss)),\n",
    "            tf.Summary.Value(\n",
    "                tag='accuracy_%s' % prefix, simple_value=float(accuracy))\n",
    "        ])\n",
    "        self.summary_writer.add_summary(summary, epoch)\n",
    "\n",
    "    def _define_inputs(self):\n",
    "        shape = [None]\n",
    "        shape.extend(self.data_shape)\n",
    "        self.images = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=shape,\n",
    "            name='input_images')\n",
    "        self.labels = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=[None, self.n_classes],\n",
    "            name='labels')\n",
    "        self.learning_rate = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=[],\n",
    "            name='learning_rate')\n",
    "        self.is_training = tf.placeholder(tf.bool, shape=[])\n",
    "\n",
    "    def composite_function(self, _input, out_features, kernel_size=3):\n",
    "        \"\"\"Function from paper H_l that performs:\n",
    "        - batch normalization\n",
    "        - ReLU nonlinearity\n",
    "        - convolution with required kernel\n",
    "        - dropout, if required\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"composite_function\"):\n",
    "            # BN\n",
    "            output = self.batch_norm(_input)\n",
    "            # ReLU\n",
    "            output = tf.nn.relu(output)\n",
    "            # convolution\n",
    "            output = self.conv2d(\n",
    "                output, out_features=out_features, kernel_size=kernel_size)\n",
    "            # dropout(in case of training and in case it is no 1.0)\n",
    "            output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "    def bottleneck(self, _input, out_features):\n",
    "        with tf.variable_scope(\"bottleneck\"):\n",
    "            output = self.batch_norm(_input)\n",
    "            output = tf.nn.relu(output)\n",
    "            inter_features = out_features * 4\n",
    "            output = self.conv2d(\n",
    "                output, out_features=inter_features, kernel_size=1,\n",
    "                padding='VALID')\n",
    "            output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "    def add_internal_layer(self, _input, growth_rate):\n",
    "        \"\"\"Perform H_l composite function for the layer and after concatenate\n",
    "        input with output from composite function.\n",
    "        \"\"\"\n",
    "        # call composite function with 3x3 kernel\n",
    "        if not self.bc_mode:\n",
    "            comp_out = self.composite_function(\n",
    "                _input, out_features=growth_rate, kernel_size=3)\n",
    "        elif self.bc_mode:\n",
    "            bottleneck_out = self.bottleneck(_input, out_features=growth_rate)\n",
    "            comp_out = self.composite_function(\n",
    "                bottleneck_out, out_features=growth_rate, kernel_size=3)\n",
    "        # concatenate _input with out from composite function\n",
    "        if TF_VERSION >= 1.0:\n",
    "            output = tf.concat(axis=3, values=(_input, comp_out))\n",
    "        else:\n",
    "            output = tf.concat(3, (_input, comp_out))\n",
    "        return output\n",
    "\n",
    "    def add_block(self, _input, growth_rate, layers_per_block):\n",
    "        \"\"\"Add N H_l internal layers\"\"\"\n",
    "        output = _input\n",
    "        for layer in range(layers_per_block):\n",
    "            with tf.variable_scope(\"layer_%d\" % layer):\n",
    "                output = self.add_internal_layer(output, growth_rate)\n",
    "        return output\n",
    "\n",
    "    def transition_layer(self, _input):\n",
    "        \"\"\"Call H_l composite function with 1x1 kernel and after average\n",
    "        pooling\n",
    "        \"\"\"\n",
    "        # call composite function with 1x1 kernel\n",
    "        out_features = int(int(_input.get_shape()[-1]) * self.reduction)\n",
    "        output = self.composite_function(\n",
    "            _input, out_features=out_features, kernel_size=1)\n",
    "        # run average pooling\n",
    "        output = self.avg_pool(output, k=2)\n",
    "        return output\n",
    "\n",
    "    def transition_layer_to_classes(self, _input):\n",
    "        \"\"\"This is last transition to get probabilities by classes. It perform:\n",
    "        - batch normalization\n",
    "        - ReLU nonlinearity\n",
    "        - wide average pooling\n",
    "        - FC layer multiplication\n",
    "        \"\"\"\n",
    "        # BN\n",
    "        output = self.batch_norm(_input)\n",
    "        # ReLU\n",
    "        output = tf.nn.relu(output)\n",
    "        # average pooling\n",
    "        last_pool_kernel = int(output.get_shape()[-2])\n",
    "        output = self.avg_pool(output, k=last_pool_kernel)\n",
    "        # FC\n",
    "        features_total = int(output.get_shape()[-1])\n",
    "        output = tf.reshape(output, [-1, features_total])\n",
    "        W = self.weight_variable_xavier(\n",
    "            [features_total, self.n_classes], name='W')\n",
    "        bias = self.bias_variable([self.n_classes])\n",
    "        logits = tf.matmul(output, W) + bias\n",
    "        return logits\n",
    "\n",
    "    def conv2d(self, _input, out_features, kernel_size,\n",
    "               strides=[1, 1, 1, 1], padding='SAME'):\n",
    "        in_features = int(_input.get_shape()[-1])\n",
    "        kernel = self.weight_variable_msra(\n",
    "            [kernel_size, kernel_size, in_features, out_features],\n",
    "            name='kernel')\n",
    "        output = tf.nn.conv2d(_input, kernel, strides, padding)\n",
    "        return output\n",
    "\n",
    "    def avg_pool(self, _input, k):\n",
    "        ksize = [1, k, k, 1]\n",
    "        strides = [1, k, k, 1]\n",
    "        padding = 'VALID'\n",
    "        output = tf.nn.avg_pool(_input, ksize, strides, padding)\n",
    "        return output\n",
    "\n",
    "    def batch_norm(self, _input):\n",
    "        output = tf.contrib.layers.batch_norm(\n",
    "            _input, scale=True, is_training=self.is_training,\n",
    "            updates_collections=None)\n",
    "        return output\n",
    "\n",
    "    def dropout(self, _input):\n",
    "        if self.keep_prob < 1:\n",
    "            output = tf.cond(\n",
    "                self.is_training,\n",
    "                lambda: tf.nn.dropout(_input, self.keep_prob),\n",
    "                lambda: _input\n",
    "            )\n",
    "        else:\n",
    "            output = _input\n",
    "        return output\n",
    "\n",
    "    def weight_variable_msra(self, shape, name):\n",
    "        return tf.get_variable(\n",
    "            name=name,\n",
    "            shape=shape,\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "\n",
    "    def weight_variable_xavier(self, shape, name):\n",
    "        return tf.get_variable(\n",
    "            name,\n",
    "            shape=shape,\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    def bias_variable(self, shape, name='bias'):\n",
    "        initial = tf.constant(0.0, shape=shape)\n",
    "        return tf.get_variable(name, initializer=initial)\n",
    "\n",
    "    def _build_graph(self):\n",
    "        growth_rate = self.growth_rate\n",
    "        layers_per_block = self.layers_per_block\n",
    "        # first - initial 3 x 3 conv to first_output_features\n",
    "        with tf.variable_scope(\"Initial_convolution\"):\n",
    "            output = self.conv2d(\n",
    "                self.images,\n",
    "                out_features=self.first_output_features,\n",
    "                kernel_size=3)\n",
    "\n",
    "        # add N required blocks\n",
    "        for block in range(self.total_blocks):\n",
    "            with tf.variable_scope(\"Block_%d\" % block):\n",
    "                output = self.add_block(output, growth_rate, layers_per_block)\n",
    "            # last block exist without transition layer\n",
    "            if block != self.total_blocks - 1:\n",
    "                with tf.variable_scope(\"Transition_after_block_%d\" % block):\n",
    "                    output = self.transition_layer(output)\n",
    "\n",
    "        with tf.variable_scope(\"Transition_to_classes\"):\n",
    "            logits = self.transition_layer_to_classes(output)\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        # Losses\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=self.labels))\n",
    "        self.cross_entropy = cross_entropy\n",
    "        l2_loss = tf.add_n(\n",
    "            [tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "\n",
    "        # optimizer and train step\n",
    "        optimizer = tf.train.MomentumOptimizer(\n",
    "            self.learning_rate, self.nesterov_momentum, use_nesterov=True)\n",
    "        self.train_step = optimizer.minimize(\n",
    "            cross_entropy + l2_loss * self.weight_decay)\n",
    "\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(prediction, 1),\n",
    "            tf.argmax(self.labels, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def train_all_epochs(self, train_params):\n",
    "        n_epochs = train_params['n_epochs']\n",
    "        learning_rate = train_params['initial_learning_rate']\n",
    "        batch_size = train_params['batch_size']\n",
    "        reduce_lr_epoch_1 = train_params['reduce_lr_epoch_1']\n",
    "        reduce_lr_epoch_2 = train_params['reduce_lr_epoch_2']\n",
    "        total_start_time = time.time()\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            print(\"\\n\", '-' * 30, \"Train epoch: %d\" % epoch, '-' * 30, '\\n')\n",
    "            start_time = time.time()\n",
    "            if epoch == reduce_lr_epoch_1 or epoch == reduce_lr_epoch_2:\n",
    "                learning_rate = learning_rate / 10\n",
    "                print(\"Decrease learning rate, new lr = %f\" % learning_rate)\n",
    "\n",
    "            print(\"Training...\")\n",
    "            loss, acc = self.train_one_epoch(\n",
    "                self.data_provider.train, batch_size, learning_rate)\n",
    "            if self.should_save_logs:\n",
    "                self.log_loss_accuracy(loss, acc, epoch, prefix='train')\n",
    "\n",
    "            if train_params.get('validation_set', False):\n",
    "                print(\"Validation...\")\n",
    "                loss, acc = self.test(\n",
    "                    self.data_provider.validation, batch_size)\n",
    "                if self.should_save_logs:\n",
    "                    self.log_loss_accuracy(loss, acc, epoch, prefix='valid')\n",
    "\n",
    "            time_per_epoch = time.time() - start_time\n",
    "            seconds_left = int((n_epochs - epoch) * time_per_epoch)\n",
    "            print(\"Time per epoch: %s, Est. complete in: %s\" % (\n",
    "                str(timedelta(seconds=time_per_epoch)),\n",
    "                str(timedelta(seconds=seconds_left))))\n",
    "\n",
    "            if self.should_save_model:\n",
    "                self.save_model()\n",
    "\n",
    "        total_training_time = time.time() - total_start_time\n",
    "        print(\"\\nTotal training time: %s\" % str(timedelta(\n",
    "            seconds=total_training_time)))\n",
    "\n",
    "    def train_one_epoch(self, data, batch_size, learning_rate):\n",
    "        num_examples = data.num_examples\n",
    "        total_loss = []\n",
    "        total_accuracy = []\n",
    "        for i in range(num_examples // batch_size):\n",
    "            batch = data.next_batch(batch_size)\n",
    "            images, labels = batch\n",
    "            feed_dict = {\n",
    "                self.images: images,\n",
    "                self.labels: labels,\n",
    "                self.learning_rate: learning_rate,\n",
    "                self.is_training: True,\n",
    "            }\n",
    "            fetches = [self.train_step, self.cross_entropy, self.accuracy]\n",
    "            result = self.sess.run(fetches, feed_dict=feed_dict)\n",
    "            _, loss, accuracy = result\n",
    "            total_loss.append(loss)\n",
    "            total_accuracy.append(accuracy)\n",
    "            if self.should_save_logs:\n",
    "                self.batches_step += 1\n",
    "                self.log_loss_accuracy(\n",
    "                    loss, accuracy, self.batches_step, prefix='per_batch',\n",
    "                    should_print=False)\n",
    "        mean_loss = np.mean(total_loss)\n",
    "        mean_accuracy = np.mean(total_accuracy)\n",
    "        return mean_loss, mean_accuracy\n",
    "\n",
    "    def test(self, data, batch_size):\n",
    "        num_examples = data.num_examples\n",
    "        total_loss = []\n",
    "        total_accuracy = []\n",
    "        for i in range(num_examples // batch_size):\n",
    "            batch = data.next_batch(batch_size)\n",
    "            feed_dict = {\n",
    "                self.images: batch[0],\n",
    "                self.labels: batch[1],\n",
    "                self.is_training: False,\n",
    "            }\n",
    "            fetches = [self.cross_entropy, self.accuracy]\n",
    "            loss, accuracy = self.sess.run(fetches, feed_dict=feed_dict)\n",
    "            total_loss.append(loss)\n",
    "            total_accuracy.append(accuracy)\n",
    "        mean_loss = np.mean(total_loss)\n",
    "        mean_accuracy = np.mean(total_accuracy)\n",
    "        return mean_loss, mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MtsFlm_xCbEs"
   },
   "source": [
    "The following is the main code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u1LHbIOOBt1N"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from models.dense_net import DenseNet\n",
    "from data_providers.utils import get_data_provider_by_name\n",
    "\n",
    "train_params_cifar = {\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 300,\n",
    "    'initial_learning_rate': 0.1,\n",
    "    'reduce_lr_epoch_1': 150,  # epochs * 0.5\n",
    "    'reduce_lr_epoch_2': 225,  # epochs * 0.75\n",
    "    'validation_set': True,\n",
    "    'validation_split': None,  # None or float\n",
    "    'shuffle': 'every_epoch',  # None, once_prior_train, every_epoch\n",
    "    'normalization': 'by_chanels',  # None, divide_256, divide_255, by_chanels\n",
    "}\n",
    "\n",
    "train_params_svhn = {\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 40,\n",
    "    'initial_learning_rate': 0.1,\n",
    "    'reduce_lr_epoch_1': 20,\n",
    "    'reduce_lr_epoch_2': 30,\n",
    "    'validation_set': True,\n",
    "    'validation_split': None,  # you may set it 6000 as in the paper\n",
    "    'shuffle': True,  # shuffle dataset every epoch or not\n",
    "    'normalization': 'divide_255',\n",
    "}\n",
    "\n",
    "\n",
    "def get_train_params_by_name(name):\n",
    "    if name in ['C10', 'C10+', 'C100', 'C100+']:\n",
    "        return train_params_cifar\n",
    "    if name == 'SVHN':\n",
    "        return train_params_svhn\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--train', action='store_true',\n",
    "        help='Train the model')\n",
    "    parser.add_argument(\n",
    "        '--test', action='store_true',\n",
    "        help='Test model for required dataset if pretrained model exists.'\n",
    "             'If provided together with `--train` flag testing will be'\n",
    "             'performed right after training.')\n",
    "    parser.add_argument(\n",
    "        '--model_type', '-m', type=str, choices=['DenseNet', 'DenseNet-BC'],\n",
    "        default='DenseNet',\n",
    "        help='What type of model to use')\n",
    "    parser.add_argument(\n",
    "        '--growth_rate', '-k', type=int, choices=[12, 24, 40],\n",
    "        default=12,\n",
    "        help='Grows rate for every layer, '\n",
    "             'choices were restricted to used in paper')\n",
    "    parser.add_argument(\n",
    "        '--depth', '-d', type=int, choices=[40, 100, 190, 250],\n",
    "        default=40,\n",
    "        help='Depth of whole network, restricted to paper choices')\n",
    "    parser.add_argument(\n",
    "        '--dataset', '-ds', type=str,\n",
    "        choices=['C10', 'C10+', 'C100', 'C100+', 'SVHN'],\n",
    "        default='C10',\n",
    "        help='What dataset should be used')\n",
    "    parser.add_argument(\n",
    "        '--total_blocks', '-tb', type=int, default=3, metavar='',\n",
    "        help='Total blocks of layers stack (default: %(default)s)')\n",
    "    parser.add_argument(\n",
    "        '--keep_prob', '-kp', type=float, metavar='',\n",
    "        help=\"Keep probability for dropout.\")\n",
    "    parser.add_argument(\n",
    "        '--weight_decay', '-wd', type=float, default=1e-4, metavar='',\n",
    "        help='Weight decay for optimizer (default: %(default)s)')\n",
    "    parser.add_argument(\n",
    "        '--nesterov_momentum', '-nm', type=float, default=0.9, metavar='',\n",
    "        help='Nesterov momentum (default: %(default)s)')\n",
    "    parser.add_argument(\n",
    "        '--reduction', '-red', type=float, default=0.5, metavar='',\n",
    "        help='reduction Theta at transition layer for DenseNets-BC models')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--logs', dest='should_save_logs', action='store_true',\n",
    "        help='Write tensorflow logs')\n",
    "    parser.add_argument(\n",
    "        '--no-logs', dest='should_save_logs', action='store_false',\n",
    "        help='Do not write tensorflow logs')\n",
    "    parser.set_defaults(should_save_logs=True)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--saves', dest='should_save_model', action='store_true',\n",
    "        help='Save model during training')\n",
    "    parser.add_argument(\n",
    "        '--no-saves', dest='should_save_model', action='store_false',\n",
    "        help='Do not save model during training')\n",
    "    parser.set_defaults(should_save_model=True)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--renew-logs', dest='renew_logs', action='store_true',\n",
    "        help='Erase previous logs for model if exists.')\n",
    "    parser.add_argument(\n",
    "        '--not-renew-logs', dest='renew_logs', action='store_false',\n",
    "        help='Do not erase previous logs for model if exists.')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--num_inter_threads', '-inter', type=int, default=1, metavar='',\n",
    "        help='number of inter threads for inference / test')\n",
    "    parser.add_argument(\n",
    "        '--num_intra_threads', '-intra', type=int, default=128, metavar='',\n",
    "        help='number of intra threads for inference / test')\n",
    "    \n",
    "    \n",
    "    parser.set_defaults(renew_logs=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not args.keep_prob:\n",
    "        if args.dataset in ['C10', 'C100', 'SVHN']:\n",
    "            args.keep_prob = 0.8\n",
    "        else:\n",
    "            args.keep_prob = 1.0\n",
    "    if args.model_type == 'DenseNet':\n",
    "        args.bc_mode = False\n",
    "        args.reduction = 1.0\n",
    "    elif args.model_type == 'DenseNet-BC':\n",
    "        args.bc_mode = True\n",
    "\n",
    "    model_params = vars(args)\n",
    "\n",
    "    if not args.train and not args.test:\n",
    "        print(\"You should train or test your network. Please check params.\")\n",
    "        exit()\n",
    "\n",
    "    # some default params dataset/architecture related\n",
    "    train_params = get_train_params_by_name(args.dataset)\n",
    "    print(\"Params:\")\n",
    "    for k, v in model_params.items():\n",
    "        print(\"\\t%s: %s\" % (k, v))\n",
    "    print(\"Train params:\")\n",
    "    for k, v in train_params.items():\n",
    "        print(\"\\t%s: %s\" % (k, v))\n",
    "\n",
    "    print(\"Prepare training data...\")\n",
    "    data_provider = get_data_provider_by_name(args.dataset, train_params)\n",
    "    print(\"Initialize the model..\")\n",
    "    model = DenseNet(data_provider=data_provider, **model_params)\n",
    "    if args.train:\n",
    "        print(\"Data provider train images: \", data_provider.train.num_examples)\n",
    "        model.train_all_epochs(train_params)\n",
    "    if args.test:\n",
    "        if not args.train:\n",
    "            model.load_model()\n",
    "        print(\"Data provider test images: \", data_provider.test.num_examples)\n",
    "        print(\"Testing...\")\n",
    "        loss, accuracy = model.test(data_provider.test, batch_size=200)\n",
    "        print(\"mean cross_entropy: %f, mean accuracy: %f\" % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GcAYXVjxz6aV"
   },
   "source": [
    "## Making this work\n",
    "\n",
    "So there are two models built in the paper: \n",
    "\n",
    "1.   just using the DenseNet encoder and then a GRU decoder\n",
    "2.   enriching the above with a multi-scale attention\n",
    "\n",
    "I think the NMT with attention from the recent homework is a good shell to replace things on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction\n",
    "\n",
    "Handwritten mathematical-expression recognition (HMR) is the problem that attempts to take a handwritten mathematical formula such as:\n",
    "\n",
    "[Insert image]\n",
    "\n",
    "and convert it into some computer-readable format.  The nearly-ubiquitous format is LaTeX encoding, which is how modern math papers are typically written.  In LaTeX, the above image would be expressed as \n",
    "\n",
    "[LaTeX code]\n",
    "\n",
    "HMR is a difficult problem because it adds several features on top of normal handwriting recognition:\n",
    "\n",
    "1.   LaTeX is a computer language with a precise syntax, so minor 'grammatical' errors will cause the output to be unreadable.  Thus HMR models must learn the hidden syntax of '{''s and '_''s from just the images and their LaTeX labels.\n",
    "2.   The same symbol(s) can be scaled and translated in different ways.  So, if the label has learned that 'cos' should be mapped to '\\cos', then it must also learn this if 'cos' is written under a square root sign, or if is smaller and written above a fraction bar.\n",
    "\n",
    "To attack this difficult problem, Zhang, Du, and Dai [paper] have recently proposed a model they describe as multi-scale attention with dense encoder.  The exact architecture is described later, but we give the outline here.  The model is an encoder-decoder model with attention.  The encoder is a convolutional encoder based on the DenseNet architecture recently introduced by NAMES [densenet].  A DenseNet augments a normal convolutional neural network by feeding the convolutional layers the output of several preceeding layers, rather than just the immediate previous one.  [densenet] showed that this architecture performs excellently for image classification tasks, so [paper] adapts it to the problem of HMR.  The decoder is a one-directional GRU decoder with multi-scale attention; that is, it uses two context vectors for attention: a 'low-resolution' version for feature extraction and a 'high-resolution' version to extract small details.\n",
    "\n",
    "The Computer Recognition of Handwritten Mathematical Expressions (CROHME) is a recurring contest to solve the HMR problem.  [paper]'s model outperforms the state-of-the-art models from the 2014 and 2016 competitions.  On the 2016 dataset, for instance, [paper] acheived a 50.1% accuracy, higher than the first place model (49.6%), even though that model used a much larger training corpus pulled from Wikipedia.\n",
    "\n",
    "We implement the model described in [paper] and attempt to replicate it's success on a dataset from Kaggle [Kaggle] that combines several HMR datasets, including CROHME 2011.  We describe the data preprocessing (both here and in `preprocessing.ipynb`), detail the implementation of the architecture, train the model, and test its performance.\n",
    "\n",
    "\n",
    "\n",
    "## Preliminary package loading\n",
    "\n",
    "The following are the necessary packages for this code (note the `tf.enable_eager_execution()` flag).  This should be run on TensorFlow 1.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "TF_VERSION = float('.'.join(tf.__version__.split('.')[:2]))\n",
    "TF_VERSION\n",
    "\n",
    "This flag sets where the paths are going.\n",
    "\n",
    "user = 'will'\n",
    "#user = 'ben'\n",
    "is_colab = True\n",
    "\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "\n",
    "\n",
    "\n",
    "## Data loading\n",
    "\n",
    "\n",
    "\n",
    "The CROHME datasets are provided as inkML files tagged with the target value of the LaTeX code.  Another file--'Parsing inkml.ipynb'--explains the file extension and the conversion from the original files to a list of dictionaries giving a 200x200 image array and the LaTeX string.\n",
    "\n",
    "Provide the location of 'savedimgs.p' as PATH, then we load it to 'content.'\n",
    "\n",
    "if is_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "\n",
    "\n",
    "\n",
    "# will: PATH = '/content/gdrive/My Drive/Colab Notebooks/MHR Final/savedimgs.p'\n",
    "PATH = '/content/gdrive/My Drive/Colab Notebooks/MHR Final/savedimgs.p'\n",
    "\n",
    "with open(PATH, 'rb') as pickle_file:\n",
    "    content = pickle.load(pickle_file)\n",
    "\n",
    "i = np.random.randint(len(content))\n",
    "\n",
    "plt.imshow(content[i]['img'])\n",
    "print(content[i]['label'])\n",
    "\n",
    "content[i]['img'].shape\n",
    "\n",
    "We separate the input parts (images) and output parts (LaTeX labels).  The images are already in the desired form, so we don't need to do anymore processing (that was done in 'Parsing inkml.ipynb'), but we will need to do more processing for the labels.\n",
    "\n",
    "X = np.asarray([content[i]['img'] for i in range(len(content))])\n",
    "Y = [content[i]['label'] for i in range(len(content))]\n",
    "\n",
    "PATH = '/content/gdrive/My Drive/Colab Notebooks/MHR Final/train_X_chunk0.p'\n",
    "\n",
    "with open(PATH, 'rb') as pickle_file:\n",
    "    X = pickle.load(pickle_file)\n",
    "    \n",
    "PATH = '/content/gdrive/My Drive/Colab Notebooks/MHR Final/train_labels.p'\n",
    "\n",
    "with open(PATH, 'rb') as pickle_file:\n",
    "    Y = pickle.load(pickle_file)\n",
    "    \n",
    "Y = Y[:1000]\n",
    "\n",
    "X[0].shape\n",
    "type(Y[0])\n",
    "\n",
    "X=np.asarray(X)\n",
    "\n",
    "## Tokenizing, etc.\n",
    "\n",
    "Here we process the labels, `Y`.  The lables are provided as strings with some additional whitespace (LaTeX's ubiquitous \\$ are omitted throughout). `tokenize` converts such a string to a list of symbols with no whitespace and with '<start>' and '<end>' tokens added.  Care must be taken because LaTeX uses \\\\ to start multicharacter commands (\\\\frac), and we want to code these as a single symbol.\n",
    "\n",
    "def tokenize(formula):\n",
    "    # takes a string of LaTeX code and breaks it into its consituent pieces\n",
    "    # also removes spaces\n",
    "    # adds '<end>' to the end and '<start>' to the start\n",
    "    as_list = ['<start>']\n",
    "    multi = False\n",
    "    first = False\n",
    "    temp = ''\n",
    "    esc_chars = ['_', '^', ' ', '{', '}', ',', '(', ')', '+', '-', '*', '\\\\','[', ']', '&', '/', '0','1','2','3','4','5','6','7','8','9']\n",
    "    weak_esc_chars = ['{','}','(',')','[',']']\n",
    "    for char in formula:\n",
    "        if not multi and char != '\\\\' and char != ' ':\n",
    "            as_list.append(char)\n",
    "        if multi and not first:\n",
    "            if char in esc_chars:\n",
    "                as_list.append(temp)\n",
    "                multi = False\n",
    "                temp = ''\n",
    "                if char != ' ':\n",
    "                    as_list.append(char)\n",
    "            else:\n",
    "                temp = temp + char\n",
    "        if multi and first:\n",
    "            if char in weak_esc_chars:\n",
    "                as_list.append(temp)\n",
    "                multi = False\n",
    "                temp = ''\n",
    "                if char != ' ':\n",
    "                    as_list.append(char)\n",
    "            else:\n",
    "                temp = temp + char\n",
    "        if char == '\\\\':\n",
    "            temp = char\n",
    "            multi = True\n",
    "            first = True\n",
    "    as_list.append('<end>')\n",
    "    return as_list\n",
    "\n",
    "`get_vocab` returns all of the symbols used\n",
    "\n",
    "def get_vocab(formulas):\n",
    "    # returns the LaTeX symbols that are used and always puts '<end>' at the start\n",
    "    vocab = ['PAD', '<end>', '<start>']\n",
    "    for formula in formulas:\n",
    "        for word in tokenize(formula):\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "    vocab.sort()\n",
    "    vocab.insert(0, vocab.pop(vocab.index('<start>')))\n",
    "    vocab.insert(0, vocab.pop(vocab.index('<end>')))\n",
    "    vocab.insert(0, vocab.pop(vocab.index('PAD')))\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(Y)\n",
    "\n",
    "This allows us to encode symbols as single numbers.  We can use `vocab[int]` to give the symbol corresponding to `int` and `vocab.index(str)` to give the number corresponding to the symbol `str`.\n",
    "\n",
    "print(len(vocab))\n",
    "print([vocab[i] for i in np.random.randint(len(vocab), size=5)])\n",
    "\n",
    "vocab\n",
    "\n",
    "def max_length_targ(formulas):\n",
    "    max_len = 0\n",
    "    for formula in formulas:\n",
    "        max_len = max(max_len, len(tokenize(formula)))\n",
    "    return max_len\n",
    "\n",
    "max_targ_fmla = max_length_targ(Y)\n",
    "\n",
    "max_targ_fmla\n",
    "\n",
    "`encode_latex` turns a formula string into a list of numbers (of constant length when `pad = True`) and `decode_latex` turns a list of numbers into the corresponding LaTeX string.\n",
    "\n",
    "def encode_latex(string, vocab, pad = False, length=0):\n",
    "    # Turns a LaTeX string into a encoded vector based on vocab ordering\n",
    "    vector = [vocab.index(char) for char in tokenize(string)]\n",
    "    if pad:\n",
    "        while len(vector) < length:\n",
    "            vector.append(0)\n",
    "    return np.asarray(vector)\n",
    "\n",
    "def decode_latex(vector, vocab,pad=False):\n",
    "    string = ''\n",
    "    vector = list(vector)\n",
    "    for num in vector:\n",
    "        string = string + vocab[num]\n",
    "        if num == 0:\n",
    "            break\n",
    "    return string\n",
    "\n",
    "def code_output(formulas, fmla_length, vocab):\n",
    "    # assumes that the formulas come in as un tokenized strings\n",
    "    coded_formulas = [encode_latex(formula,vocab, pad = True,length=fmla_length) for formula in formulas]\n",
    "    return np.asarray(coded_formulas)\n",
    "\n",
    "Finally, we normalize, reshape, and retype the arrays.\n",
    "\n",
    "# Turns out BatchNormalization doesn't like 'float64'\n",
    "X=X.astype('float32')\n",
    "\n",
    "X\n",
    "\n",
    "#X = X//255.\n",
    "X_coded = np.reshape(X, (X.shape[0],X.shape[1],X.shape[2],1))\n",
    "\n",
    "Y_coded = code_output(Y, max_targ_fmla, vocab)\n",
    "\n",
    "print(X_coded.shape, Y_coded.shape)\n",
    "\n",
    "plt.imshow(X_coded[5,:,:,0])\n",
    "\n",
    "Now that we've put the data in the right shape, we should split it into train and test sets:\n",
    "\n",
    "X_train = X_coded\n",
    "Y_train = Y_coded\n",
    "\n",
    "## As tf.dataset\n",
    "\n",
    "BUFFER_SIZE = len(X_train) #\n",
    "BATCH_SIZE = 8 # More and it crashes!\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_coded, Y_coded)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# DenseNet Encoder with Multi-Scale Attention\n",
    "\n",
    "We implement a model based of [paper], a DenseNet Encoder with Multi-Scale Attention.  There are two main novel features to this model that we want to explore:\n",
    "\n",
    "\n",
    "1.   The use of DenseNets (with Bottlenecks) in the encoder\n",
    "2.   The use of multi-scale attention in the decoder\n",
    "\n",
    "We explain the idea behind these notions in the relevant section.  The implementation is based on a combination of of [K],[nmt],[paper].\n",
    "\n",
    "\n",
    "\n",
    "## Prologue\n",
    "\n",
    "We set several global variables to use in building and training the model.  These will be explained when used.  CP_PATH is the file path for checkpoints of the model.\n",
    "\n",
    "\n",
    "### Some parameters\n",
    "\n",
    "CP_PATH = '/content/gdrive/My Drive/Colab Notebooks/MHR Final/training_checkpoints/DNB_Bahd'\n",
    "\n",
    "## From the paper\n",
    "\n",
    "TOTAL_BLOCKS = 3\n",
    "GROWTH_RATE = 24 # denoted k in paper\n",
    "DEPTH = 32 # denoted D in paper\n",
    "DROPOUT = 0.2 # fraction to dropout\n",
    "\n",
    "ATTENTION_GROWTH_RATE = 24\n",
    "ATTENTION_DEPTH = 16\n",
    "\n",
    "EMBEDDING_DIM = 32 # 256 in paper, reduced due to size of dataset\n",
    "GRU_UNITS = 128 # 256 in paper, reduced due to size of dataset\n",
    "\n",
    "\n",
    "## From the dataset\n",
    "\n",
    "\n",
    "INPUT_SHAPE = X_train[0].shape[1:] ##\n",
    "IS_TRAINING = True\n",
    "OUTPUT_SIZE = len(vocab) # number of symbols in LaTeX vocabulary\n",
    "\n",
    "MAX_LENGTH_TARG = max_targ_fmla # How long we want to run the predictor until we force it to end\n",
    "\n",
    "TARG_START_TOKEN = '<start>'\n",
    "TARG_END_TOKEN = '<end>'\n",
    "\n",
    "\n",
    "\n",
    "## Fine-tuning the model\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "We also use the gru wrapper from [nmt] to automatically detect a GPU.\n",
    "\n",
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    if tf.test.is_gpu_available(): \n",
    "  # if gpu: \n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "\n",
    "## Encoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### DenseNet Architecture\n",
    "\n",
    "\n",
    "We build the DenseNet encoder from [paper] in Keras.  The DenseNet architecture was developed in [DenseNet], and our implementation owes much to the TensorFlow implementation developed by K-- [K].  The overall architecture of the encoder is:\n",
    "\n",
    "\n",
    "1.   Image input\n",
    "2.   Initial 7x7 convolution with 2x2 and 2x2 max-pooling\n",
    "3.   DenseNetB block\n",
    "4.   Transition layer (convolution and average-pool)\n",
    "5.   DenseNetB block\n",
    "6.   Transition layer\n",
    "7.   DenseNetB block\n",
    "\n",
    "\n",
    "\n",
    "The main feature of the encoder are the DenseNet blocks, which take in parameters $k$ for growth rate and $D$ for depth.  The blocks are labeled DenseNetB because they include bottlenecks.\n",
    "\n",
    "Each DenseNetB block is made up of several 3x3 convolutional layers (with normalization/activation/etc.), with a twist.  Typically, convlutional layers are done in sequence, with the input to each layer being the output to the previous layer:\n",
    "\n",
    "$L_0 = \\text{Conv}(\\text{input})$\n",
    "\n",
    "$L_1 = \\text{Conv}(L_0)$\n",
    "\n",
    "$L_2 = \\text{Conv}(L_1)$\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "However, in DenseNetB block, the first layer remains the same:\n",
    "\n",
    "$L^{DN}_0 = \\text{Conv}(\\text{input})$\n",
    "\n",
    "Then, each additional layer takes as input all convolutional outputs concatenated together:\n",
    "\n",
    "$L^{DN}_1 = \\text{Conv}(L^{DN}_0)$\n",
    "\n",
    "$L^{DN}_2 = \\text{Conv}(L^{DN}_0, L^{DN}_1)$\n",
    "\n",
    "$L^{DN}_3 = \\text{Conv}(L^{DN}_0, L^{DN}_1, L^{DN}_2)$\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "This does XXXX\n",
    "\n",
    "Between each DenseNetB block, a transition layer is added consising of a 1x1 convolutional layer and a 2x2 average pooling layer.  This cuts the number of channels in half and reduces dimensionality to keep the size of thr layers under control.\n",
    "\n",
    "Up until now, we have described the structure of the original DenseNet block.  The bottleneck layer were a layer addition that improves computational efficiency by reducing the number of channels.  The bottleneck layer is similar in structure to a layer of DenseNet, but the convolutional layer is 1x1 and reduces the number of filters.\n",
    "\n",
    "\n",
    "\n",
    "### Multi-scale Attention\n",
    "\n",
    "The decoder will be a single GRU layer.  This means that it will take in a context vector.  However, just taking the output of the DNB leads to resolution errors.  That is, the DenseNet output ends up being squashed to a low-resolution context vector.  This is fine for some things, but in mathematical formulas, there are small features--small enough to be confused for noise--that end up greatly affecting the meaning of the expression.  A perfect example (provided in [paper]) is the decimal point in '3.000003.'  A low-resoluion context vector might completely miss the '.', leading to massive change in the intended meaning.  \n",
    "\n",
    "To fix this, [paper] employs multi-scale attention by forking another path off from the middle of the transition layer 6 (before pooling occurs).  This output is then put through another DenseNet block (although with differed depth) to provide a more high-resolution context.  The two context vectors are then separately fed into an attention mechanism, and the results are concatenated (see the implementation of the DNBDecoder).    This high resolution image will allow the model to spot the import decimal point in the example above.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "We now work through the specifics of the parameter calls.  The parameters are given default values (from [paper] or even [densenet]).\n",
    "\n",
    "class DNBEncoder(tf.keras.Model):\n",
    "    ## implements the DenseNet with Bottlenecks encoder from paper\n",
    "    ## Update to actual variables we want\n",
    "    def __init__(self,\n",
    "                 inp_shape = INPUT_SHAPE,\n",
    "                 dropout_frac = DROPOUT,\n",
    "                 total_blocks = TOTAL_BLOCKS,\n",
    "                 depth = DEPTH,\n",
    "                 attention_growth_rate = ATTENTION_GROWTH_RATE,\n",
    "                 attention_depth = ATTENTION_DEPTH,\n",
    "                 growth_rate = GROWTH_RATE,\n",
    "                 enc_units = GRU_UNITS,\n",
    "                 batch_size = BATCH_SIZE):\n",
    "        super(DNBEncoder, self).__init__()\n",
    "        self.inp_shape = inp_shape\n",
    "        self.dropout_frac = dropout_frac\n",
    "        self.total_blocks = total_blocks\n",
    "        self.depth = depth\n",
    "        self.attention_growth_rate = attention_growth_rate\n",
    "        self.attention_depth = attention_depth\n",
    "        self.growth_rate = growth_rate\n",
    "        self.enc_units = GRU_UNITS\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        # New variables\n",
    "        self.gru = gru(enc_units)\n",
    "                \n",
    "    def call(self, inp, hidden, is_training):\n",
    "        bias = False\n",
    "        \n",
    "        # Initial Conv2D layer: kernel reads 7 x 7, strides are 2 x 2, and the number of output channels is 48\n",
    "        x = tf.keras.layers.Conv2D(48, (7,7), strides = (2,2), input_shape = self.inp_shape, use_bias = bias)(inp)\n",
    "        \n",
    "        \n",
    "        # Feed into a MaxPooling layer\n",
    "        x = self.add_maxpool(x, k=2)\n",
    "        \n",
    "        for block in range(self.total_blocks):\n",
    "            if block > 0:\n",
    "            # This puts a transition layer between each of the blocks\n",
    "                x = self.add_conv2d(x, spot_size=1, num_output_channels = int(self.growth_rate//2))\n",
    "                if block == self.total_blocks - 1:\n",
    "                # Peels off in the middle of the final transition layer to have a high-resolution context vector\n",
    "                    y = self.add_DNB(x, self.attention_growth_rate, is_training, bias, depth=self.attention_depth)\n",
    "                x = self.add_avgpool(x, k=2)\n",
    "            # Adds the DenseNetB block\n",
    "            x = self.add_DNB(x, self.growth_rate, is_training, bias, depth=self.depth)\n",
    "            \n",
    "        \n",
    "        # Reshapes the outputs to feed into the GRU decoder, turning (-,a,b, c) into (-, a*b, c)\n",
    "        #x = tf.keras.layers.Reshape((144, 36))(x)\n",
    "        #y = tf.keras.layers.Reshape((576,36))(y)\n",
    "        x = tf.keras.layers.Reshape((x.shape[1]*x.shape[2],x.shape[3]))(x)\n",
    "        y = tf.keras.layers.Reshape((y.shape[1]*y.shape[2],y.shape[3]))(y)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def add_conv2d(self, inp, spot_size, num_output_channels, stride = 1, bias = True, padding = 'SAME'):\n",
    "        # Adds a 2D convolutional layer with a spotlight size of spot_size x spot_size and a stride of stride x stride that has num_output_channels many output features\n",
    "        x = tf.keras.layers.Conv2D(num_output_channels, kernel_size = spot_size, strides = stride, use_bias = bias, padding = padding)(inp)\n",
    "        return x\n",
    "                \n",
    "    def add_maxpool(self, inp, k):\n",
    "        x = tf.keras.layers.MaxPool2D(pool_size=k)(inp)\n",
    "        return x\n",
    "        \n",
    "    def add_avgpool(self, inp, k):\n",
    "        x = tf.keras.layers.AveragePooling2D(pool_size=k)(inp)\n",
    "        return x\n",
    "        \n",
    "    def add_DNB(self, inp, growth_rate, is_training, bias, depth):\n",
    "        x = inp\n",
    "        layers_per_block = (depth - (self.total_blocks + 1)) // self.total_blocks\n",
    "        for layer in range(layers_per_block):\n",
    "            x = self.add_bottleneck(inp, growth_rate, is_training, bias=bias, padding='VALID')\n",
    "            x = self.add_DN(x, output_features = growth_rate, kernel_size = 3, is_training = is_training, bias=bias)\n",
    "            x = tf.concat(axis=3, values=(inp, x))\n",
    "        return x\n",
    "    \n",
    "    def add_bottleneck(self, inp, growth_rate, is_training, bias, padding='VALID'):\n",
    "        x = tf.keras.layers.BatchNormalization()(inp, is_training)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.add_conv2d(x, num_output_channels = 4 * growth_rate, spot_size = 1, bias=bias, padding=padding)\n",
    "        x = self.add_dropout(x, is_training)\n",
    "        return(x)\n",
    "    \n",
    "    def add_dropout(self, inp, is_training):\n",
    "        if self.dropout_frac < 1:\n",
    "            x = tf.keras.layers.Dropout(self.dropout_frac)(inp, is_training)\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def add_DN(self, inp, output_features, kernel_size = 3, bias=False, is_training = True):\n",
    "        x = self.add_comp(inp, output_features, kernel_size = kernel_size, bias=bias, is_training=is_training)\n",
    "        return x\n",
    "    \n",
    "    def add_comp(self, inp, out_features, kernel_size=3, bias=False, is_training = True):\n",
    "        x = tf.keras.layers.BatchNormalization()(inp, is_training)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.add_conv2d(x, num_output_channels = out_features, spot_size = kernel_size, bias = bias)\n",
    "        x = self.add_dropout(x, is_training)\n",
    "        return x        \n",
    "            \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))\n",
    "\n",
    "## Decoder\n",
    "\n",
    "\n",
    "The decoder is, at its base, a standard GRU encoder.  This will allow us to predict the next symbol in the formula based on the encoder output and the previous (predicted symbol).  It is augmented by adding the multi-scale attention as mentioned above.  Calling the decoder includes providing the low-resolution and high-resolution outputs of the encoder.  These are then put through a Bahdanau attention layer separately, and then concatenated with the previous value to be put through the GRU decoder.\n",
    "\n",
    "We return the attention weights even though we do nothing with them.  Future work could implement an attention grapher that displays where the model is paying attention at each symbol (the paper implements such a graph).\n",
    "\n",
    "[Insert Fig]\n",
    "\n",
    "class DNBDecoder(tf.keras.Model):\n",
    "    def __init__(self, target_language_size=OUTPUT_SIZE, embedding_dim=EMBEDDING_DIM, dec_units=GRU_UNITS, batch_size = BATCH_SIZE):\n",
    "        super(DNBDecoder, self).__init__()\n",
    "        self.target_language_size = target_language_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.batch_size = batch_size\n",
    "        # New variables\n",
    "        self.embedding = tf.keras.layers.Embedding(self.target_language_size, self.embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(self.target_language_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1_low = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2_low = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V_low = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        self.W1_high = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2_high = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V_high = tf.keras.layers.Dense(1)\n",
    "\n",
    "        \n",
    "    def call(self, inp, hidden, low_output, high_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score_low = self.V_low(tf.nn.tanh(self.W1_low(low_output) + self.W2_low(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights_low = tf.nn.softmax(score_low, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector_low = attention_weights_low * low_output\n",
    "        context_vector_low = tf.reduce_sum(context_vector_low, axis=1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score_high = self.V_high(tf.nn.tanh(self.W1_high(high_output) + self.W2_high(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights_high = tf.nn.softmax(score_high, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector_high = attention_weights_high * high_output\n",
    "        context_vector_high = tf.reduce_sum(context_vector_high, axis=1)\n",
    "\n",
    "        context_vector = tf.concat([context_vector_low, context_vector_high], axis=1)\n",
    "        \n",
    "        #print('inp:',inp)\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(inp)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        #print('emb:',x)\n",
    "        #print('context:',context_vector)\n",
    "        #print('expand:', tf.expand_dims(context_vector,1))\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        #x = tf.concat([context_vector, x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights_low, attention_weights_high#, context_vector_low, context_vector_high\n",
    "\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.dec_units))\n",
    "\n",
    "## Epilogue\n",
    "\n",
    "\n",
    "\n",
    "We add in the optimizers and loss.  We follow [paper] and use Adadelta optimization.\n",
    "\n",
    "optimizer = tf.train.AdadeltaOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "Here we instantiate the encoder and decoder.  Everything runs off default parameters.\n",
    "\n",
    "encoder = DNBEncoder()\n",
    "decoder = DNBDecoder()\n",
    "\n",
    "encoder.inp_shape\n",
    "\n",
    "Now, let's add checkpointing.  This will save checkpoints to the folder CP_PATH specified above.\n",
    "\n",
    "checkpoint_dir = CP_PATH\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "# Training\n",
    "\n",
    "\n",
    "\n",
    "The following is the training routine that runs for EPOCHS many epochs.  We use teacher forcing and base the algorithm on the one in [nmt].  Note that there is no GRU in the encoder, so it produces no hidden state.  Instead, we use an initialized hidden state to start the decoder, and keep feeding it back to itself (as in [paper]).\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            print(batch)\n",
    "            enc_output_low, enc_output_high = encoder(inp, hidden, is_training=True)\n",
    "            dec_hidden = hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([vocab.index('<start>')] * BATCH_SIZE, 1)       \n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, dec_hidden, _, _ = decoder(dec_input, dec_hidden, enc_output_low, enc_output_high)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    print('Estimated seconds to completion: {}'.format((EPOCH-epoch-1)*(time.time()-start)))\n",
    "\n",
    "# Evaluating\n",
    "\n",
    "\n",
    "First, let's reload the last checkpoint in case you don't want to rerun the training everytime.\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "The following functions give the LaTeX predictions for an input image by feeding the image into the encoder, and then seeing what the decoder decodes.  It is important to set `is_training = False` to tell the BatchNormalization and Dropout layers that this is a prediction run, not a training run.\n",
    "\n",
    "def evaluate(image, encoder, decoder, latex_vocab=vocab, max_length_targ = MAX_LENGTH_TARG):\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = tf.zeros((BATCH_SIZE, GRU_UNITS))\n",
    "\n",
    "    enc_out_low, enc_out_high = encoder(image, hidden, is_training=False)\n",
    "\n",
    "    dec_hidden = hidden\n",
    "    dec_input = tf.expand_dims([vocab.index('<start>')] * BATCH_SIZE, 1)       \n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, _, _ = decoder(dec_input, dec_hidden, enc_out_low, enc_out_high)#, is_training=False)\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += latex_vocab[predicted_id]\n",
    "\n",
    "        if latex_vocab[predicted_id] == '<end>':\n",
    "            return result, image\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id] * BATCH_SIZE,1)\n",
    "        \n",
    "    return result, image\n",
    "\n",
    "def texify(image, encoder = encoder, decoder = decoder, latex_vocab=vocab, max_length_targ = MAX_LENGTH_TARG):\n",
    "    result,_ = evaluate(np.reshape(image,(1,200,200,1)), encoder, decoder, latex_vocab, max_length_targ)\n",
    "    print(result)\n",
    "    print(\"should represent\")\n",
    "    plt.imshow(image[:,:,0])#display_image(image)\n",
    "\n",
    "image = X_re[np.random.randint(len(X_re))]\n",
    "texify(image)\n",
    "\n",
    "### Validating\n",
    "\n",
    "We should have split the data set into test and train, so let's evaluate the appropriate metric on our models\n",
    "\n",
    "# Epilogue\n",
    "\n",
    "In which we draw conclusions about how we deserve an A\n",
    "\n",
    "# References\n",
    "\n",
    "\n",
    "\n",
    "1.   Paper we implement: https://arxiv.org/abs/1801.03530\n",
    "2.   DenseNet original: https://arxiv.org/abs/1608.06993\n",
    "3.   Khlestov implementation: [here](https://github.com/ikhlestov/vision_networks) for the github, and [here](https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#.55qu3tfqm) for more commentary\n",
    "4.    CROHME data set:\n",
    "5.    nmt_with_attention:\n",
    "6.    nmt_with_attention(class_version):\n",
    "7.    Bahdanau attention:\n",
    "\n",
    "\n",
    "\n",
    "# Old stuff and Notes\n",
    "\n",
    "## TensorFlow implementation\n",
    "\n",
    "The following is/is based on the TensorFlow implementation by Illarion Khlestov.  See [here](https://github.com/ikhlestov/vision_networks) for the github, and [here](https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#.55qu3tfqm) for more commentary.\n",
    "\n",
    "This has the following requirements: ipdb\n",
    "ipython\n",
    "matplotlib\n",
    "numpy\n",
    "Pillow\n",
    "scipy\n",
    "\n",
    "and either tensorflow>=0.10.0\n",
    " or tensorflow-gpu>=0.10.0\n",
    "\n",
    "The following is the building of DenseNet (models.dense_net in the main)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "TF_VERSION = float('.'.join(tf.__version__.split('.')[:2]))\n",
    "\n",
    "\n",
    "class DenseNet:\n",
    "    def __init__(self, data_provider, growth_rate, depth,\n",
    "                 total_blocks, keep_prob, num_inter_threads, num_intra_threads,\n",
    "                 weight_decay, nesterov_momentum, model_type, dataset,\n",
    "                 should_save_logs, should_save_model,\n",
    "                 renew_logs=False,\n",
    "                 reduction=1.0,\n",
    "                 bc_mode=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Class to implement networks from this paper\n",
    "        https://arxiv.org/pdf/1611.05552.pdf\n",
    "        Args:\n",
    "            data_provider: Class, that have all required data sets\n",
    "            growth_rate: `int`, variable from paper\n",
    "            depth: `int`, variable from paper\n",
    "            total_blocks: `int`, paper value == 3\n",
    "            keep_prob: `float`, keep probability for dropout. If keep_prob = 1\n",
    "                dropout will be disables\n",
    "            weight_decay: `float`, weight decay for L2 loss, paper = 1e-4\n",
    "            nesterov_momentum: `float`, momentum for Nesterov optimizer\n",
    "            model_type: `str`, 'DenseNet' or 'DenseNet-BC'. Should model use\n",
    "                bottle neck connections or not.\n",
    "            dataset: `str`, dataset name\n",
    "            should_save_logs: `bool`, should logs be saved or not\n",
    "            should_save_model: `bool`, should model be saved or not\n",
    "            renew_logs: `bool`, remove previous logs for current model\n",
    "            reduction: `float`, reduction Theta at transition layer for\n",
    "                DenseNets with bottleneck layers. See paragraph 'Compression'\n",
    "                https://arxiv.org/pdf/1608.06993v3.pdf#4\n",
    "            bc_mode: `bool`, should we use bottleneck layers and features\n",
    "                reduction or not.\n",
    "        \"\"\"\n",
    "        self.data_provider = data_provider\n",
    "        self.data_shape = data_provider.data_shape\n",
    "        self.n_classes = data_provider.n_classes\n",
    "        self.depth = depth\n",
    "        self.growth_rate = growth_rate\n",
    "        self.num_inter_threads = num_inter_threads\n",
    "        self.num_intra_threads = num_intra_threads\n",
    "        # how many features will be received after first convolution\n",
    "        # value the same as in the original Torch code\n",
    "        self.first_output_features = growth_rate * 2\n",
    "        self.total_blocks = total_blocks\n",
    "        self.layers_per_block = (depth - (total_blocks + 1)) // total_blocks\n",
    "        self.bc_mode = bc_mode\n",
    "        # compression rate at the transition layers\n",
    "        self.reduction = reduction\n",
    "        if not bc_mode:\n",
    "            print(\"Build %s model with %d blocks, \"\n",
    "                  \"%d composite layers each.\" % (\n",
    "                      model_type, self.total_blocks, self.layers_per_block))\n",
    "        if bc_mode:\n",
    "            self.layers_per_block = self.layers_per_block // 2\n",
    "            print(\"Build %s model with %d blocks, \"\n",
    "                  \"%d bottleneck layers and %d composite layers each.\" % (\n",
    "                      model_type, self.total_blocks, self.layers_per_block,\n",
    "                      self.layers_per_block))\n",
    "        print(\"Reduction at transition layers: %.1f\" % self.reduction)\n",
    "\n",
    "        self.keep_prob = keep_prob\n",
    "        self.weight_decay = weight_decay\n",
    "        self.nesterov_momentum = nesterov_momentum\n",
    "        self.model_type = model_type\n",
    "        self.dataset_name = dataset\n",
    "        self.should_save_logs = should_save_logs\n",
    "        self.should_save_model = should_save_model\n",
    "        self.renew_logs = renew_logs\n",
    "        self.batches_step = 0\n",
    "\n",
    "        self._define_inputs()\n",
    "        self._build_graph()\n",
    "        self._initialize_session()\n",
    "        self._count_trainable_params()\n",
    "\n",
    "    def _initialize_session(self):\n",
    "        \"\"\"Initialize session, variables, saver\"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "\n",
    "        # Specify the CPU inter and Intra threads used by MKL\n",
    "        config.intra_op_parallelism_threads = self.num_intra_threads\n",
    "        config.inter_op_parallelism_threads = self.num_inter_threads\n",
    "\n",
    "        # restrict model GPU memory utilization to min required\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        tf_ver = int(tf.__version__.split('.')[1])\n",
    "        if TF_VERSION <= 0.10:\n",
    "            self.sess.run(tf.initialize_all_variables())\n",
    "            logswriter = tf.train.SummaryWriter\n",
    "        else:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            logswriter = tf.summary.FileWriter\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.summary_writer = logswriter(self.logs_path)\n",
    "\n",
    "    def _count_trainable_params(self):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "            shape = variable.get_shape()\n",
    "            variable_parametes = 1\n",
    "            for dim in shape:\n",
    "                variable_parametes *= dim.value\n",
    "            total_parameters += variable_parametes\n",
    "        print(\"Total training params: %.1fM\" % (total_parameters / 1e6))\n",
    "\n",
    "    @property\n",
    "    def save_path(self):\n",
    "        try:\n",
    "            save_path = self._save_path\n",
    "        except AttributeError:\n",
    "            save_path = 'saves/%s' % self.model_identifier\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            save_path = os.path.join(save_path, 'model.chkpt')\n",
    "            self._save_path = save_path\n",
    "        return save_path\n",
    "\n",
    "    @property\n",
    "    def logs_path(self):\n",
    "        try:\n",
    "            logs_path = self._logs_path\n",
    "        except AttributeError:\n",
    "            logs_path = 'logs/%s' % self.model_identifier\n",
    "            if self.renew_logs:\n",
    "                shutil.rmtree(logs_path, ignore_errors=True)\n",
    "            os.makedirs(logs_path, exist_ok=True)\n",
    "            self._logs_path = logs_path\n",
    "        return logs_path\n",
    "\n",
    "    @property\n",
    "    def model_identifier(self):\n",
    "        return \"{}_growth_rate={}_depth={}_dataset_{}\".format(\n",
    "            self.model_type, self.growth_rate, self.depth, self.dataset_name)\n",
    "\n",
    "    def save_model(self, global_step=None):\n",
    "        self.saver.save(self.sess, self.save_path, global_step=global_step)\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.saver.restore(self.sess, self.save_path)\n",
    "        except Exception as e:\n",
    "            raise IOError(\"Failed to to load model \"\n",
    "                          \"from save path: %s\" % self.save_path)\n",
    "        self.saver.restore(self.sess, self.save_path)\n",
    "        print(\"Successfully load model from save path: %s\" % self.save_path)\n",
    "\n",
    "    def log_loss_accuracy(self, loss, accuracy, epoch, prefix,\n",
    "                          should_print=True):\n",
    "        if should_print:\n",
    "            print(\"mean cross_entropy: %f, mean accuracy: %f\" % (\n",
    "                loss, accuracy))\n",
    "        summary = tf.Summary(value=[\n",
    "            tf.Summary.Value(\n",
    "                tag='loss_%s' % prefix, simple_value=float(loss)),\n",
    "            tf.Summary.Value(\n",
    "                tag='accuracy_%s' % prefix, simple_value=float(accuracy))\n",
    "        ])\n",
    "        self.summary_writer.add_summary(summary, epoch)\n",
    "\n",
    "    def _define_inputs(self):\n",
    "        shape = [None]\n",
    "        shape.extend(self.data_shape)\n",
    "        self.images = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=shape,\n",
    "            name='input_images')\n",
    "        self.labels = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=[None, self.n_classes],\n",
    "            name='labels')\n",
    "        self.learning_rate = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=[],\n",
    "            name='learning_rate')\n",
    "        self.is_training = tf.placeholder(tf.bool, shape=[])\n",
    "\n",
    "    def composite_function(self, _input, out_features, kernel_size=3):\n",
    "        \"\"\"Function from paper H_l that performs:\n",
    "        - batch normalization\n",
    "        - ReLU nonlinearity\n",
    "        - convolution with required kernel\n",
    "        - dropout, if required\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"composite_function\"):\n",
    "            # BN\n",
    "            output = self.batch_norm(_input)\n",
    "            # ReLU\n",
    "            output = tf.nn.relu(output)\n",
    "            # convolution\n",
    "            output = self.conv2d(\n",
    "                output, out_features=out_features, kernel_size=kernel_size)\n",
    "            # dropout(in case of training and in case it is no 1.0)\n",
    "            output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "    def bottleneck(self, _input, out_features):\n",
    "        with tf.variable_scope(\"bottleneck\"):\n",
    "            output = self.batch_norm(_input)\n",
    "            output = tf.nn.relu(output)\n",
    "            inter_features = out_features * 4\n",
    "            output = self.conv2d(\n",
    "                output, out_features=inter_features, kernel_size=1,\n",
    "                padding='VALID')\n",
    "            output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "    def add_internal_layer(self, _input, growth_rate):\n",
    "        \"\"\"Perform H_l composite function for the layer and after concatenate\n",
    "        input with output from composite function.\n",
    "        \"\"\"\n",
    "        # call composite function with 3x3 kernel\n",
    "        if not self.bc_mode:\n",
    "            comp_out = self.composite_function(\n",
    "                _input, out_features=growth_rate, kernel_size=3)\n",
    "        elif self.bc_mode:\n",
    "            bottleneck_out = self.bottleneck(_input, out_features=growth_rate)\n",
    "            comp_out = self.composite_function(\n",
    "                bottleneck_out, out_features=growth_rate, kernel_size=3)\n",
    "        # concatenate _input with out from composite function\n",
    "        if TF_VERSION >= 1.0:\n",
    "            output = tf.concat(axis=3, values=(_input, comp_out))\n",
    "        else:\n",
    "            output = tf.concat(3, (_input, comp_out))\n",
    "        return output\n",
    "\n",
    "    def add_block(self, _input, growth_rate, layers_per_block):\n",
    "        \"\"\"Add N H_l internal layers\"\"\"\n",
    "        output = _input\n",
    "        for layer in range(layers_per_block):\n",
    "            with tf.variable_scope(\"layer_%d\" % layer):\n",
    "                output = self.add_internal_layer(output, growth_rate)\n",
    "        return output\n",
    "\n",
    "    def transition_layer(self, _input):\n",
    "        \"\"\"Call H_l composite function with 1x1 kernel and after average\n",
    "        pooling\n",
    "        \"\"\"\n",
    "        # call composite function with 1x1 kernel\n",
    "        out_features = int(int(_input.get_shape()[-1]) * self.reduction)\n",
    "        output = self.composite_function(\n",
    "            _input, out_features=out_features, kernel_size=1)\n",
    "        # run average pooling\n",
    "        output = self.avg_pool(output, k=2)\n",
    "        return output\n",
    "\n",
    "    def transition_layer_to_classes(self, _input):\n",
    "        \"\"\"This is last transition to get probabilities by classes. It perform:\n",
    "        - batch normalization\n",
    "        - ReLU nonlinearity\n",
    "        - wide average pooling\n",
    "        - FC layer multiplication\n",
    "        \"\"\"\n",
    "        # BN\n",
    "        output = self.batch_norm(_input)\n",
    "        # ReLU\n",
    "        output = tf.nn.relu(output)\n",
    "        # average pooling\n",
    "        last_pool_kernel = int(output.get_shape()[-2])\n",
    "        output = self.avg_pool(output, k=last_pool_kernel)\n",
    "        # FC\n",
    "        features_total = int(output.get_shape()[-1])\n",
    "        output = tf.reshape(output, [-1, features_total])\n",
    "        W = self.weight_variable_xavier(\n",
    "            [features_total, self.n_classes], name='W')\n",
    "        bias = self.bias_variable([self.n_classes])\n",
    "        logits = tf.matmul(output, W) + bias\n",
    "        return logits\n",
    "\n",
    "    def conv2d(self, _input, out_features, kernel_size,\n",
    "               strides=[1, 1, 1, 1], padding='SAME'):\n",
    "        in_features = int(_input.get_shape()[-1])\n",
    "        kernel = self.weight_variable_msra(\n",
    "            [kernel_size, kernel_size, in_features, out_features],\n",
    "            name='kernel')\n",
    "        output = tf.nn.conv2d(_input, kernel, strides, padding)\n",
    "        return output\n",
    "\n",
    "    def avg_pool(self, _input, k):\n",
    "        ksize = [1, k, k, 1]\n",
    "        strides = [1, k, k, 1]\n",
    "        padding = 'VALID'\n",
    "        output = tf.nn.avg_pool(_input, ksize, strides, padding)\n",
    "        return output\n",
    "\n",
    "    def batch_norm(self, _input):\n",
    "        output = tf.contrib.layers.batch_norm(\n",
    "            _input, scale=True, is_training=self.is_training,\n",
    "            updates_collections=None)\n",
    "        return output\n",
    "\n",
    "    def dropout(self, _input):\n",
    "        if self.keep_prob < 1:\n",
    "            output = tf.cond(\n",
    "                self.is_training,\n",
    "                lambda: tf.nn.dropout(_input, self.keep_prob),\n",
    "                lambda: _input\n",
    "            )\n",
    "        else:\n",
    "            output = _input\n",
    "        return output\n",
    "\n",
    "    def weight_variable_msra(self, shape, name):\n",
    "        return tf.get_variable(\n",
    "            name=name,\n",
    "            shape=shape,\n",
    "            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "\n",
    "    def weight_variable_xavier(self, shape, name):\n",
    "        return tf.get_variable(\n",
    "            name,\n",
    "            shape=shape,\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    def bias_variable(self, shape, name='bias'):\n",
    "        initial = tf.constant(0.0, shape=shape)\n",
    "        return tf.get_variable(name, initializer=initial)\n",
    "\n",
    "    def _build_graph(self):\n",
    "        growth_rate = self.growth_rate\n",
    "        layers_per_block = self.layers_per_block\n",
    "        # first - initial 3 x 3 conv to first_output_features\n",
    "        with tf.variable_scope(\"Initial_convolution\"):\n",
    "            output = self.conv2d(\n",
    "                self.images,\n",
    "                out_features=self.first_output_features,\n",
    "                kernel_size=3)\n",
    "\n",
    "        # add N required blocks\n",
    "        for block in range(self.total_blocks):\n",
    "            with tf.variable_scope(\"Block_%d\" % block):\n",
    "                output = self.add_block(output, growth_rate, layers_per_block)\n",
    "            # last block exist without transition layer\n",
    "            if block != self.total_blocks - 1:\n",
    "                with tf.variable_scope(\"Transition_after_block_%d\" % block):\n",
    "                    output = self.transition_layer(output)\n",
    "\n",
    "        with tf.variable_scope(\"Transition_to_classes\"):\n",
    "            logits = self.transition_layer_to_classes(output)\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        # Losses\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=self.labels))\n",
    "        self.cross_entropy = cross_entropy\n",
    "        l2_loss = tf.add_n(\n",
    "            [tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "\n",
    "        # optimizer and train step\n",
    "        optimizer = tf.train.MomentumOptimizer(\n",
    "            self.learning_rate, self.nesterov_momentum, use_nesterov=True)\n",
    "        self.train_step = optimizer.minimize(\n",
    "            cross_entropy + l2_loss * self.weight_decay)\n",
    "\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(prediction, 1),\n",
    "            tf.argmax(self.labels, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def train_all_epochs(self, train_params):\n",
    "        n_epochs = train_params['n_epochs']\n",
    "        learning_rate = train_params['initial_learning_rate']\n",
    "        batch_size = train_params['batch_size']\n",
    "        reduce_lr_epoch_1 = train_params['reduce_lr_epoch_1']\n",
    "        reduce_lr_epoch_2 = train_params['reduce_lr_epoch_2']\n",
    "        total_start_time = time.time()\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            print(\"\\n\", '-' * 30, \"Train epoch: %d\" % epoch, '-' * 30, '\\n')\n",
    "            start_time = time.time()\n",
    "            if epoch == reduce_lr_epoch_1 or epoch == reduce_lr_epoch_2:\n",
    "                learning_rate = learning_rate / 10\n",
    "                print(\"Decrease learning rate, new lr = %f\" % learning_rate)\n",
    "\n",
    "            print(\"Training...\")\n",
    "            loss, acc = self.train_one_epoch(\n",
    "                self.data_provider.train, batch_size, learning_rate)\n",
    "            if self.should_save_logs:\n",
    "                self.log_loss_accuracy(loss, acc, epoch, prefix='train')\n",
    "\n",
    "            if train_params.get('validation_set', False):\n",
    "                print(\"Validation...\")\n",
    "                loss, acc = self.test(\n",
    "                    self.data_provider.validation, batch_size)\n",
    "                if self.should_save_logs:\n",
    "                    self.log_loss_accuracy(loss, acc, epoch, prefix='valid')\n",
    "\n",
    "            time_per_epoch = time.time() - start_time\n",
    "            seconds_left = int((n_epochs - epoch) * time_per_epoch)\n",
    "            print(\"Time per epoch: %s, Est. complete in: %s\" % (\n",
    "                str(timedelta(seconds=time_per_epoch)),\n",
    "                str(timedelta(seconds=seconds_left))))\n",
    "\n",
    "            if self.should_save_model:\n",
    "                self.save_model()\n",
    "\n",
    "        total_training_time = time.time() - total_start_time\n",
    "        print(\"\\nTotal training time: %s\" % str(timedelta(\n",
    "            seconds=total_training_time)))\n",
    "\n",
    "    def train_one_epoch(self, data, batch_size, learning_rate):\n",
    "        num_examples = data.num_examples\n",
    "        total_loss = []\n",
    "        total_accuracy = []\n",
    "        for i in range(num_examples // batch_size):\n",
    "            batch = data.next_batch(batch_size)\n",
    "            images, labels = batch\n",
    "            feed_dict = {\n",
    "                self.images: images,\n",
    "                self.labels: labels,\n",
    "                self.learning_rate: learning_rate,\n",
    "                self.is_training: True,\n",
    "            }\n",
    "            fetches = [self.train_step, self.cross_entropy, self.accuracy]\n",
    "            result = self.sess.run(fetches, feed_dict=feed_dict)\n",
    "            _, loss, accuracy = result\n",
    "            total_loss.append(loss)\n",
    "            total_accuracy.append(accuracy)\n",
    "            if self.should_save_logs:\n",
    "                self.batches_step += 1\n",
    "                self.log_loss_accuracy(\n",
    "                    loss, accuracy, self.batches_step, prefix='per_batch',\n",
    "                    should_print=False)\n",
    "        mean_loss = np.mean(total_loss)\n",
    "        mean_accuracy = np.mean(total_accuracy)\n",
    "        return mean_loss, mean_accuracy\n",
    "\n",
    "    def test(self, data, batch_size):\n",
    "        num_examples = data.num_examples\n",
    "        total_loss = []\n",
    "        total_accuracy = []\n",
    "        for i in range(num_examples // batch_size):\n",
    "            batch = data.next_batch(batch_size)\n",
    "            feed_dict = {\n",
    "                self.images: batch[0],\n",
    "                self.labels: batch[1],\n",
    "                self.is_training: False,\n",
    "            }\n",
    "            fetches = [self.cross_entropy, self.accuracy]\n",
    "            loss, accuracy = self.sess.run(fetches, feed_dict=feed_dict)\n",
    "            total_loss.append(loss)\n",
    "            total_accuracy.append(accuracy)\n",
    "        mean_loss = np.mean(total_loss)\n",
    "        mean_accuracy = np.mean(total_accuracy)\n",
    "        return mean_loss, mean_accuracy\n",
    "\n",
    "The following is the main code:\n",
    "\n",
    "import argparse\n",
    "\n",
    "from models.dense_net import DenseNet\n",
    "from data_providers.utils import get_data_provider_by_name\n",
    "\n",
    "train_params_cifar = {\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 300,\n",
    "    'initial_learning_rate': 0.1,\n",
    "    'reduce_lr_epoch_1': 150,  # epochs * 0.5\n",
    "    'reduce_lr_epoch_2': 225,  # epochs * 0.75\n",
    "    'validation_set': True,\n",
    "    'validation_split': None,  # None or float\n",
    "    'shuffle': 'every_epoch',  # None, once_prior_train, every_epoch\n",
    "    'normalization': 'by_chanels',  # None, divide_256, divide_255, by_chanels\n",
    "}\n",
    "\n",
    "train_params_svhn = {\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 40,\n",
    "    'initial_learning_rate': 0.1,\n",
    "    'reduce_lr_epoch_1': 20,\n",
    "    'reduce_lr_epoch_2': 30,\n",
    "    'validation_set': True,\n",
    "    'validation_split': None,  # you may set it 6000 as in the paper\n",
    "    'shuffle': True,  # shuffle dataset every epoch or not\n",
    "    'normalization': 'divide_255',\n",
    "}\n",
    "\n",
    "\n",
    "def get_train_params_by_name(name):\n",
    "    if name in ['C10', 'C10+', 'C100', 'C100+']:\n",
    "        return train_params_cifar\n",
    "    if name == 'SVHN':\n",
    "        return train_params_svhn\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--train', action='store_true',\n",
    "        help='Train the model')\n",
    "    parser.add_argument(\n",
    "        '--test', action='store_true',\n",
    "        help='Test model for required dataset if pretrained model exists.'\n",
    "             'If provided together with `--train` flag testing will be'\n",
    "             'performed right after training.')\n",
    "    parser.add_argument(\n",
    "        '--model_type', '-m', type=str, choices=['DenseNet', 'DenseNet-BC'],\n",
    "        default='DenseNet',\n",
    "        help='What type of model to use')\n",
    "    parser.add_argument(\n",
    "        '--growth_rate', '-k', type=int, choices=[12, 24, 40],\n",
    "        default=12,\n",
    "        help='Grows rate for every layer, '\n",
    "             'choices were restricted to used in paper')\n",
    "    parser.add_argument(\n",
    "        '--depth', '-d', type=int, choices=[40, 100, 190, 250],\n",
    "        default=40,\n",
    "        help='Depth of whole network, restricted to paper choices')\n",
    "    parser.add_argument(\n",
    "        '--dataset', '-ds', type=str,\n",
    "        choices=['C10', 'C10+', 'C100', 'C100+', 'SVHN'],\n",
    "        default='C10',\n",
    "        help='What dataset should be used')\n",
    "    parser.add_argument(\n",
    "        '--total_blocks', '-tb', type=int, default=3, metavar='',\n",
    "        help='Total blocks of layers stack (default: %(default)s)')\n",
    "    parser.add_argument(\n",
    "        '--keep_prob', '-kp', type=float, metavar='',\n",
    "        help=\"Keep probability for dropout.\")\n",
    "    parser.add_argument(\n",
    "        '--weight_decay', '-wd', type=float, default=1e-4, metavar='',\n",
    "        help='Weight decay for optimizer (default: %(default)s)')\n",
    "    parser.add_argument(\n",
    "        '--nesterov_momentum', '-nm', type=float, default=0.9, metavar='',\n",
    "        help='Nesterov momentum (default: %(default)s)')\n",
    "    parser.add_argument(\n",
    "        '--reduction', '-red', type=float, default=0.5, metavar='',\n",
    "        help='reduction Theta at transition layer for DenseNets-BC models')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--logs', dest='should_save_logs', action='store_true',\n",
    "        help='Write tensorflow logs')\n",
    "    parser.add_argument(\n",
    "        '--no-logs', dest='should_save_logs', action='store_false',\n",
    "        help='Do not write tensorflow logs')\n",
    "    parser.set_defaults(should_save_logs=True)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--saves', dest='should_save_model', action='store_true',\n",
    "        help='Save model during training')\n",
    "    parser.add_argument(\n",
    "        '--no-saves', dest='should_save_model', action='store_false',\n",
    "        help='Do not save model during training')\n",
    "    parser.set_defaults(should_save_model=True)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--renew-logs', dest='renew_logs', action='store_true',\n",
    "        help='Erase previous logs for model if exists.')\n",
    "    parser.add_argument(\n",
    "        '--not-renew-logs', dest='renew_logs', action='store_false',\n",
    "        help='Do not erase previous logs for model if exists.')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--num_inter_threads', '-inter', type=int, default=1, metavar='',\n",
    "        help='number of inter threads for inference / test')\n",
    "    parser.add_argument(\n",
    "        '--num_intra_threads', '-intra', type=int, default=128, metavar='',\n",
    "        help='number of intra threads for inference / test')\n",
    "    \n",
    "    \n",
    "    parser.set_defaults(renew_logs=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not args.keep_prob:\n",
    "        if args.dataset in ['C10', 'C100', 'SVHN']:\n",
    "            args.keep_prob = 0.8\n",
    "        else:\n",
    "            args.keep_prob = 1.0\n",
    "    if args.model_type == 'DenseNet':\n",
    "        args.bc_mode = False\n",
    "        args.reduction = 1.0\n",
    "    elif args.model_type == 'DenseNet-BC':\n",
    "        args.bc_mode = True\n",
    "\n",
    "    model_params = vars(args)\n",
    "\n",
    "    if not args.train and not args.test:\n",
    "        print(\"You should train or test your network. Please check params.\")\n",
    "        exit()\n",
    "\n",
    "    # some default params dataset/architecture related\n",
    "    train_params = get_train_params_by_name(args.dataset)\n",
    "    print(\"Params:\")\n",
    "    for k, v in model_params.items():\n",
    "        print(\"\\t%s: %s\" % (k, v))\n",
    "    print(\"Train params:\")\n",
    "    for k, v in train_params.items():\n",
    "        print(\"\\t%s: %s\" % (k, v))\n",
    "\n",
    "    print(\"Prepare training data...\")\n",
    "    data_provider = get_data_provider_by_name(args.dataset, train_params)\n",
    "    print(\"Initialize the model..\")\n",
    "    model = DenseNet(data_provider=data_provider, **model_params)\n",
    "    if args.train:\n",
    "        print(\"Data provider train images: \", data_provider.train.num_examples)\n",
    "        model.train_all_epochs(train_params)\n",
    "    if args.test:\n",
    "        if not args.train:\n",
    "            model.load_model()\n",
    "        print(\"Data provider test images: \", data_provider.test.num_examples)\n",
    "        print(\"Testing...\")\n",
    "        loss, accuracy = model.test(data_provider.test, batch_size=200)\n",
    "        print(\"mean cross_entropy: %f, mean accuracy: %f\" % (loss, accuracy))\n",
    "\n",
    "## Making this work\n",
    "\n",
    "So there are two models built in the paper: \n",
    "\n",
    "1.   just using the DenseNet encoder and then a GRU decoder\n",
    "2.   enriching the above with a multi-scale attention\n",
    "\n",
    "I think the NMT with attention from the recent homework is a good shell to replace things on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
